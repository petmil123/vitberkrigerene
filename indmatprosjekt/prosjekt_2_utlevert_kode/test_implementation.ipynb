{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test om koden er riktig implementert\n",
    "\n",
    "Her er et forslag til testfunksjoner for å sjekke om koden er riktig implementert.\n",
    "```assert variabel``` vil gi en feilmelding med mindre variabelen ```variabel = True```. For eksempel vil ```assert a == b``` gi en feilmelding med mindre ```a``` og ```b``` er like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eksempel:\n",
    "variable = True\n",
    "assert variable, \"You need to change 'variable' to True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 4 1 0]\n",
      " [7 4 0 5 4]\n",
      " [6 5 7 7 2]\n",
      " [6 1 4 4 2]\n",
      " [0 5 0 5 7]\n",
      " [4 2 4 4 7]]\n",
      "[[5 6 1 7 1 3 3]\n",
      " [1 5 0 3 0 2 2]\n",
      " [7 6 1 5 7 7 7]\n",
      " [6 4 1 7 1 5 6]\n",
      " [1 6 1 5 2 1 6]\n",
      " [5 4 7 5 7 1 7]]\n",
      "softmax input shape: (6, 5, 5)\n",
      "softmax input shape: (6, 8, 5)\n",
      "big z: [[0.12865772 0.12252745 0.12045068 0.11636366 0.12084251]\n",
      " [0.12210292 0.12562365 0.1340768  0.11965655 0.123948  ]\n",
      " [0.12112009 0.12457227 0.11981894 0.12056444 0.12256235]\n",
      " [0.13171911 0.12588557 0.13534263 0.13524617 0.13283516]\n",
      " [0.11719596 0.12186541 0.12735415 0.12607371 0.12617547]\n",
      " [0.12142782 0.12123568 0.13337525 0.13416313 0.13194341]\n",
      " [0.13034176 0.12857848 0.11661605 0.12756998 0.12230866]\n",
      " [0.12743462 0.12971149 0.11296551 0.12036236 0.11938443]]\n",
      "z: [3 7 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "#We choose some arbitrary values for the dimensions\n",
    "b = 6\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 3\n",
    "p = 20\n",
    "\n",
    "#Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "#initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "attention = Attention(d,k) # Changed from k to n to n_max\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "\n",
    "\n",
    "#a manual forward pass\n",
    "X = onehot(x, m)\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "z = np.argmax(Z,axis=1)\n",
    "print(f\"big z: {Z[0]}\")\n",
    "print(f\"z: {z[0]}\")\n",
    "\n",
    "#check the shapes\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "assert z0.shape == (b,d,n), f\"z0.shape={z0.shape}, expected {(b,d,n)}\"\n",
    "assert z1.shape == (b,d,n), f\"z1.shape={z1.shape}, expected {(b,d,n)}\"\n",
    "assert z2.shape == (b,d,n), f\"z2.shape={z2.shape}, expected {(b,d,n)}\"\n",
    "assert z3.shape == (b,m,n), f\"z3.shape={z3.shape}, expected {(b,m,n)}\"\n",
    "assert Z.shape == (b,m,n), f\"Z.shape={Z.shape}, expected {(b,m,n)}\"\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f\"X.sum()={X.sum()}, expected {b*n}\"\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f\"Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}\"\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f\"Z.sum()={Z.sum()}, expected {b*n}\"\n",
    "assert np.all(Z>=0), f\"Z={Z}, expected all entries to be non-negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 6 4 5 6]\n",
      " [7 2 2 7 0 7]\n",
      " [0 4 3 2 3 2]\n",
      " [6 3 1 0 2 0]\n",
      " [7 2 5 6 6 1]\n",
      " [2 6 7 1 0 7]]\n",
      "softmax input shape: (6, 6, 6)\n",
      "softmax input shape: (6, 8, 6)\n",
      "(6, 8, 6)\n",
      "(6, 6)\n",
      "6\n",
      "onehotshape: (6, 8, 6)\n",
      "(6, 8, 6)\n",
      "z_l:(6, 8, 6)\n",
      "grad: (6, 8, 6)\n",
      "P: (6, 8, 6)\n",
      "Q: (6, 1, 6)\n",
      "z_l:(6, 6, 6)\n",
      "grad: (6, 6, 6)\n",
      "P: (6, 6, 6)\n",
      "Q: (6, 1, 6)\n",
      "wpd shape: (10, 6)\n",
      "grad shape: (6, 10, 6)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isinstance expected 2 arguments, got 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m _ \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mbackward(grad_Z)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#and and do a gradient descent step\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_gd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\petar\\Documents\\vitberkrigerene\\indmatprosjekt\\prosjekt_2_utlevert_kode\\neural_network.py:39\u001b[0m, in \u001b[0;36mNeuralNetwork.step_gd\u001b[1;34m(self, alpha)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03mPerform a gradient descent step for each layer,\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03mbut only if it is of the class LinearLayer.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m#Check if layer is of class a class that has parameters\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mLinearLayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mEmbedPosition\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFeedForward\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAttention\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     40\u001b[0m         layer\u001b[38;5;241m.\u001b[39mstep_gd(alpha)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: isinstance expected 2 arguments, got 5"
     ]
    }
   ],
   "source": [
    "\n",
    "#test the forward pass\n",
    "x = np.random.randint(0, m, (b,n_max))\n",
    "X = onehot(x, m)\n",
    "\n",
    "#we test with a y that is shorter than the maximum length\n",
    "#COMMENT 2P&1P: WTF!? Skal ikke disse alltid være like lange???????\n",
    "n_y = n_max - 1\n",
    "y = np.random.randint(0, m, (b,n_y))\n",
    "print(y)\n",
    "\n",
    "#initialize a neural network based on the layers above\n",
    "network = NeuralNetwork([embed_pos, feed_forward, attention, un_embed, softmax])\n",
    "#and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#do a forward pass\n",
    "Z = network.forward(X[:,:,:-1])\n",
    "\n",
    "#compute the loss\n",
    "print(Z.shape)\n",
    "print(y.shape)\n",
    "L = loss.forward(Z, y)\n",
    "\n",
    "#get the derivative of the loss wrt Z\n",
    "grad_Z = loss.backward()\n",
    "print(grad_Z.shape)\n",
    "\n",
    "#and perform a backward pass\n",
    "_ = network.backward(grad_Z)\n",
    "\n",
    "#and and do a gradient descent step\n",
    "_ = network.step_gd(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if loss is non-negative\n",
    "assert L >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.14942525, -0.        , -0.        ],\n",
       "        [-0.        , -0.66666665, -0.        ],\n",
       "        [-0.        , -0.        , -0.        ],\n",
       "        [-0.        , -0.        , -0.        ],\n",
       "        [-0.        , -0.        , -1.04166663]],\n",
       "\n",
       "       [[-0.33333333, -0.        , -0.        ],\n",
       "        [-0.        , -0.33333333, -0.        ],\n",
       "        [-0.        , -0.        , -0.        ],\n",
       "        [-0.        , -0.        , -0.        ],\n",
       "        [-0.        , -0.        , -0.33333333]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.array([[[.29,.15,.10],\n",
    "     [.21,.5,.04],\n",
    "     [.15,.03,.11],\n",
    "     [.13,.21,.43],\n",
    "     [.22,.11,.32]],\n",
    "     [\n",
    "     [1,0,0],\n",
    "     [0,1,0],\n",
    "     [0,0,0],\n",
    "     [0,0,0],\n",
    "     [0,0,1]]])\n",
    "\n",
    "\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "loss.forward(m,np.array([[0,1,4],[0,1,4]]))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ True  True]\n",
      "  [ True  True]]\n",
      "\n",
      " [[False  True]\n",
      "  [ True False]]]\n",
      "[[[False  True]\n",
      "  [ True False]]\n",
      "\n",
      " [[ True  True]\n",
      "  [ True  True]]]\n",
      "[[[2. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 2.]]]\n"
     ]
    }
   ],
   "source": [
    "x_T = np.array([[[0,0,1],\n",
    "      [0,0,0]]\n",
    "     ,[[0,0,0],\n",
    "       [1,0,0]]])\n",
    "\n",
    "a = np.ones((3,2))\n",
    "b = a.T\n",
    "\n",
    "s = np.einsum('bad,ds,sq,bqk -> bak',x_T,a, b, np.transpose(x_T, axes=(0,2,1)), optimize=True)\n",
    "for i in range(len(x_T)):\n",
    "    print(s == (x_T[i]@a)@(b@x_T[i].T))\n",
    "\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

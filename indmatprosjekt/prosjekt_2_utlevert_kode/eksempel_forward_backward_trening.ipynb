{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_addition\n",
    "\n",
    "#dimensjoner og størrelser til x og y\n",
    "n_digits = 2\n",
    "n_max = 3*n_digits\n",
    "m = 10\n",
    "\n",
    "#definerer størrelsen på parametermatrisene\n",
    "d = 15\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "#henter treningsdata\n",
    "data = get_train_test_addition(n_digits,samples_per_batch=100,n_batches_train=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import onehot\n",
    "\n",
    "x = data['x_train'][0]\n",
    "X = onehot(x,m)\n",
    "y = data['y_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 100, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['x_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import LinearLayer,EmbedPosition,Attention,Softmax,CrossEntropy,FeedForward\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vi ser at dLdW_2 tilhørende ff1 (først finner vi det lineære laget l2\n",
    "#så nøkkel 'w' i params dict-en, så nøkkel 'd' for å finne den deriverte)\n",
    "#nå er denne bare en nullmatrise \n",
    "ff1.l2.params['w']['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"manuelt\" forward pass (tilsvarende algoritme 1)\n",
    "z0 = embed.forward(X)\n",
    "z1 = att1.forward(z0)\n",
    "z2 = ff1.forward(z1)\n",
    "z = un_embed.forward(z2)\n",
    "Z = softmax.forward(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.304645857015904\n"
     ]
    }
   ],
   "source": [
    "#evaluerer objektfunksjonen\n",
    "L = loss.forward(Z,y)\n",
    "print(L)\n",
    "\n",
    "#finner den deriverte av objektfunksjonen mhp Z\n",
    "dLdZ = loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"manuelt\" backward pass (tilsvarende algoritme 2)\n",
    "dLdz = softmax.backward(dLdZ)\n",
    "dLdz2 = un_embed.backward(dLdz)\n",
    "dLdz1 = ff1.backward(dLdz2)\n",
    "dLdz0 = att1.backward(dLdz1)\n",
    "embed.backward(dLdz0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.11316356e-05,  1.90495525e-04, -4.53043131e-05,\n",
       "         1.04102072e-04, -4.20189329e-05, -1.57720936e-05,\n",
       "        -9.42090136e-05,  2.11193267e-05, -2.13131487e-04,\n",
       "         9.53218341e-05,  6.31796020e-06, -3.30341854e-05,\n",
       "        -6.48944261e-05,  6.93939535e-07,  8.99382560e-05,\n",
       "        -9.96417970e-05, -1.05875476e-05,  9.92815656e-05,\n",
       "         4.80592310e-05, -1.33901685e-04],\n",
       "       [ 4.36927303e-04,  7.05000577e-04,  2.06093563e-04,\n",
       "         1.71735628e-05,  5.06520510e-05,  3.63758179e-04,\n",
       "         1.38025295e-04, -6.18198169e-05,  4.62118627e-04,\n",
       "         7.70310270e-04, -1.03197190e-04,  1.55790681e-05,\n",
       "         3.34174859e-05, -9.60224903e-06,  2.72935228e-04,\n",
       "         3.95620957e-04,  2.19579983e-04,  2.99569717e-04,\n",
       "         6.58022094e-04,  3.64209057e-04],\n",
       "       [-4.19543271e-04, -5.94562079e-04,  1.24432226e-04,\n",
       "         5.21030661e-07,  9.49472893e-05,  1.40615544e-04,\n",
       "         3.98729384e-04, -9.90411940e-05,  4.11647503e-04,\n",
       "        -3.14338740e-04, -4.25877213e-05,  6.29199614e-06,\n",
       "        -1.17022432e-04, -2.81571342e-05,  1.77921807e-05,\n",
       "         9.92837478e-05,  7.85333021e-05, -2.05688535e-04,\n",
       "        -8.97473948e-05,  3.55877258e-04],\n",
       "       [-8.43495596e-05, -3.42197789e-04, -3.48354562e-05,\n",
       "         6.40853108e-05,  2.15281829e-05, -1.65151819e-04,\n",
       "         7.50164456e-05,  7.44433531e-05, -7.77667133e-05,\n",
       "        -6.16178154e-04,  1.04679911e-04,  2.70852553e-05,\n",
       "         1.95983577e-05,  1.69050069e-05, -8.72213761e-05,\n",
       "        -2.08413140e-04, -1.57344502e-05, -2.20163533e-04,\n",
       "        -3.10259479e-04, -1.54551061e-04],\n",
       "       [ 5.21962687e-04,  5.34560179e-04,  8.11094899e-04,\n",
       "         4.56803908e-04,  3.38355866e-04,  5.91508846e-04,\n",
       "         1.16234929e-03,  1.91511549e-04,  1.52154148e-03,\n",
       "         1.79763271e-05,  2.89248596e-04,  8.27222625e-05,\n",
       "         5.55186942e-05, -4.84579382e-06,  5.50682325e-04,\n",
       "         4.24256377e-04,  6.80281581e-04,  1.93772143e-04,\n",
       "         1.07247831e-03,  1.28367817e-03],\n",
       "       [ 4.05799097e-04,  4.06105426e-04,  5.64365655e-04,\n",
       "         2.32309755e-04,  2.40250755e-04,  4.96323820e-04,\n",
       "         8.11926599e-04,  5.97987434e-05,  1.26029923e-03,\n",
       "         3.27504344e-04,  1.12414149e-04,  8.45113219e-05,\n",
       "         1.69442195e-04, -5.77899361e-06,  3.57689590e-04,\n",
       "         4.72296373e-04,  4.79455307e-04,  1.41299221e-04,\n",
       "         7.97576279e-04,  1.00334422e-03],\n",
       "       [-1.76405340e-04,  1.16579924e-04, -1.46910145e-04,\n",
       "        -1.77744488e-04, -8.42233573e-05, -6.87930931e-06,\n",
       "        -2.65563004e-04, -1.29760042e-05, -2.14987888e-04,\n",
       "         4.71081300e-04, -7.59510102e-05, -4.57034556e-05,\n",
       "        -3.19749157e-05, -1.43166571e-05, -2.26756317e-04,\n",
       "         7.58790962e-05, -1.36944762e-04, -2.96141396e-05,\n",
       "         8.00989707e-05, -2.63929133e-05],\n",
       "       [ 5.01879335e-04,  4.42079397e-04,  4.30852238e-04,\n",
       "         2.45274477e-04,  1.94262702e-04,  3.26102287e-04,\n",
       "         5.34402359e-04,  1.31301538e-04,  7.92676290e-04,\n",
       "        -4.25154279e-05,  1.85147697e-04,  3.08511209e-05,\n",
       "        -6.94479932e-06,  7.15175874e-06,  2.64049491e-04,\n",
       "         2.77340906e-04,  3.39596095e-04,  1.96116831e-04,\n",
       "         5.33578135e-04,  5.76729969e-04],\n",
       "       [ 1.38592832e-04,  1.43862950e-04,  1.31377827e-05,\n",
       "        -2.55287165e-04, -9.46244232e-06, -1.25742204e-04,\n",
       "        -2.25213206e-04,  4.30464459e-05, -1.96499573e-04,\n",
       "        -1.71816657e-04,  3.52437857e-05,  2.95859075e-05,\n",
       "        -1.55065024e-05,  1.05706336e-05, -9.20405599e-05,\n",
       "         7.31088900e-06, -5.60457148e-05,  8.60097464e-05,\n",
       "         4.44979751e-05, -1.42117450e-04],\n",
       "       [ 2.98282983e-04,  3.15926890e-04,  2.82465027e-04,\n",
       "         7.98247094e-05,  1.09118417e-04,  7.09029128e-05,\n",
       "         2.81283237e-04,  1.39753180e-04,  3.95180694e-04,\n",
       "        -7.54087333e-05,  1.79221179e-04,  2.58483686e-05,\n",
       "         5.25392797e-05,  1.48212445e-05,  1.08844989e-04,\n",
       "         9.15146244e-05,  1.97594753e-04,  1.23098161e-04,\n",
       "         3.71336414e-04,  3.48795324e-04],\n",
       "       [ 2.59560776e-04,  2.00381843e-04,  9.46733030e-05,\n",
       "         2.04297055e-04,  1.89240969e-05,  7.59773765e-05,\n",
       "         7.75384744e-05,  9.01249033e-05,  1.15924464e-04,\n",
       "         3.05490640e-06,  9.22713493e-05,  6.68655154e-05,\n",
       "         1.60401650e-04,  5.62859148e-06,  1.78342351e-04,\n",
       "        -4.06743317e-05,  8.64136158e-05,  1.11225870e-04,\n",
       "         1.12882364e-04,  8.67337213e-05],\n",
       "       [-5.57953853e-04, -6.95780146e-04, -2.61898277e-04,\n",
       "        -1.36118312e-04, -9.44604478e-05, -2.85048669e-04,\n",
       "        -3.08156348e-04,  1.04671189e-05, -6.45664318e-04,\n",
       "        -6.59684491e-04,  1.60743469e-05, -1.91206976e-05,\n",
       "        -1.68583965e-04, -1.54595779e-05, -2.80358747e-04,\n",
       "        -3.43127607e-04, -2.91040913e-04, -2.49431039e-04,\n",
       "        -6.01183648e-04, -4.08092021e-04],\n",
       "       [-2.17138810e-04, -2.14397054e-04, -2.83558643e-05,\n",
       "         2.42030077e-04, -2.07937487e-05, -9.81587272e-05,\n",
       "         6.38094782e-05,  6.00556206e-05, -1.74603251e-04,\n",
       "        -1.62597371e-04,  1.04861914e-04, -5.76939598e-05,\n",
       "        -5.54513311e-05, -4.54547769e-06,  4.31707824e-05,\n",
       "        -2.50225723e-04, -5.37010801e-05,  1.35109947e-05,\n",
       "        -1.87575178e-04, -2.22762518e-05],\n",
       "       [-4.24613929e-06, -5.00333446e-05, -5.89646127e-05,\n",
       "        -7.96404646e-05,  9.38187741e-06,  3.81778387e-05,\n",
       "         3.30123207e-05, -8.38281334e-05,  9.59519888e-05,\n",
       "         3.11329161e-05, -1.05300932e-04, -1.12420084e-05,\n",
       "        -3.98275985e-05,  4.53632086e-06, -6.56484801e-05,\n",
       "         1.06058826e-04,  2.30343606e-05, -1.42086793e-04,\n",
       "        -4.31999170e-05, -6.61602409e-05],\n",
       "       [-4.16961201e-04, -5.08860608e-04, -1.42257166e-04,\n",
       "        -5.58910550e-05, -6.20310982e-05, -2.51801794e-04,\n",
       "        -1.06608754e-04, -4.61417217e-05, -5.49388378e-04,\n",
       "        -6.94539011e-04, -2.77170695e-05, -1.11821914e-05,\n",
       "        -2.18454606e-04, -6.92114026e-06,  2.67774394e-05,\n",
       "        -3.76326080e-04, -9.79673521e-05, -1.78761500e-04,\n",
       "        -3.55100356e-04, -3.83114454e-04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#etter backward pass har dLdW_2 fått verdier\n",
    "ff1.l2.params['w']['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import NeuralNetwork\n",
    "\n",
    "#vi kan samle lagene i en liste som vi bruker for å\n",
    "#initialisere et nevralt nettverk der vi kan bruke forward() og backward() \n",
    "#for å oppnå det samme som vi gjorde manuelt over\n",
    "\n",
    "layers = [embed,att1,ff1,un_embed,softmax]\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.304645857015904\n"
     ]
    }
   ],
   "source": [
    "#forward pass tilsvarende algoritme 1\n",
    "Z = nn.forward(X)\n",
    "\n",
    "#beregner loss med CrossEntropy\n",
    "L = loss.forward(Z,y)\n",
    "print(L)\n",
    "\n",
    "#backward pass tilsvarende algoritme 2\n",
    "dLdZ = loss.backward()\n",
    "nn.backward(dLdZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005562762195827531\n"
     ]
    }
   ],
   "source": [
    "#før optimering er W_2[0,0] gitt ved\n",
    "#ff1 er det tredje laget i layers-listen (derav layers[2])\n",
    "W_2_pre_opt = nn.layers[2].l2.params['w']['w'].copy()\n",
    "print(W_2_pre_opt[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "xs = data['x_train']\n",
    "ys = data['y_train']\n",
    "\n",
    "n_batches = xs.shape[0]\n",
    "n_iters = 100\n",
    "step_size = 0.1\n",
    "\n",
    "#treningsløkke tilsvarende algoritme 4 (med gradient descent)\n",
    "for j in range(n_iters):\n",
    "    losses = []\n",
    "    for i in range(n_batches):\n",
    "        x = xs[i]\n",
    "        y = ys[i]\n",
    "\n",
    "        X = onehot(x,m)\n",
    "        Z = nn.forward(X)\n",
    "\n",
    "        losses.append(loss.forward(Z,y))\n",
    "        dLdZ = loss.backward()\n",
    "        nn.backward(dLdZ)\n",
    "        nn.step_gd(step_size)\n",
    "    mean_loss = np.mean(losses)\n",
    "    print(\"Iterasjon \", str(j), \" L = \",mean_loss, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  1.748032448205358 \n",
      "Iterasjon  1  L =  1.7460115516598977 \n",
      "Iterasjon  2  L =  1.743930165232553 \n",
      "Iterasjon  3  L =  1.742361186610189 \n",
      "Iterasjon  4  L =  1.7409170612123994 \n",
      "Iterasjon  5  L =  1.7399883774571343 \n",
      "Iterasjon  6  L =  1.7385921623667862 \n",
      "Iterasjon  7  L =  1.7379723922021646 \n",
      "Iterasjon  8  L =  1.7368855080395864 \n",
      "Iterasjon  9  L =  1.735579187314603 \n",
      "Iterasjon  10  L =  1.7347745922216367 \n",
      "Iterasjon  11  L =  1.7337520567100932 \n",
      "Iterasjon  12  L =  1.7326801685144435 \n",
      "Iterasjon  13  L =  1.73190831917644 \n",
      "Iterasjon  14  L =  1.7308272311849935 \n",
      "Iterasjon  15  L =  1.7297996362033512 \n",
      "Iterasjon  16  L =  1.729019599610047 \n",
      "Iterasjon  17  L =  1.7279435634662186 \n",
      "Iterasjon  18  L =  1.7268959539300512 \n",
      "Iterasjon  19  L =  1.7262402402959804 \n",
      "Iterasjon  20  L =  1.7250336492004068 \n",
      "Iterasjon  21  L =  1.724276098854821 \n",
      "Iterasjon  22  L =  1.7233860507467709 \n",
      "Iterasjon  23  L =  1.7220511450697442 \n",
      "Iterasjon  24  L =  1.7214492014402616 \n",
      "Iterasjon  25  L =  1.720471559922063 \n",
      "Iterasjon  26  L =  1.7195935157155822 \n",
      "Iterasjon  27  L =  1.718493958568843 \n",
      "Iterasjon  28  L =  1.7178217253669694 \n",
      "Iterasjon  29  L =  1.7168054076871426 \n",
      "Iterasjon  30  L =  1.715769963408894 \n",
      "Iterasjon  31  L =  1.7150929721876964 \n",
      "Iterasjon  32  L =  1.7141402781505 \n",
      "Iterasjon  33  L =  1.7129115937839927 \n",
      "Iterasjon  34  L =  1.7122131766733033 \n",
      "Iterasjon  35  L =  1.7115905130948081 \n",
      "Iterasjon  36  L =  1.7104502028960153 \n",
      "Iterasjon  37  L =  1.709418606054081 \n",
      "Iterasjon  38  L =  1.7088923860858178 \n",
      "Iterasjon  39  L =  1.7081823778511678 \n",
      "Iterasjon  40  L =  1.7069595875960106 \n",
      "Iterasjon  41  L =  1.7064041075901937 \n",
      "Iterasjon  42  L =  1.7049871052562955 \n",
      "Iterasjon  43  L =  1.7045876769244488 \n",
      "Iterasjon  44  L =  1.703981420882119 \n",
      "Iterasjon  45  L =  1.7027403408303534 \n",
      "Iterasjon  46  L =  1.7012391233927031 \n",
      "Iterasjon  47  L =  1.7012613164951524 \n",
      "Iterasjon  48  L =  1.700369206684827 \n",
      "Iterasjon  49  L =  1.6983355224361194 \n",
      "Iterasjon  50  L =  1.698718427611641 \n",
      "Iterasjon  51  L =  1.6970018855701574 \n",
      "Iterasjon  52  L =  1.6954249831512962 \n",
      "Iterasjon  53  L =  1.6954684508157563 \n",
      "Iterasjon  54  L =  1.6929820513951013 \n",
      "Iterasjon  55  L =  1.6924437048153884 \n",
      "Iterasjon  56  L =  1.6911154989108161 \n",
      "Iterasjon  57  L =  1.6897431079313119 \n",
      "Iterasjon  58  L =  1.6894171807335712 \n",
      "Iterasjon  59  L =  1.687292873253115 \n",
      "Iterasjon  60  L =  1.6862944527825432 \n",
      "Iterasjon  61  L =  1.685838405578781 \n",
      "Iterasjon  62  L =  1.684434560363313 \n",
      "Iterasjon  63  L =  1.6836399534508888 \n",
      "Iterasjon  64  L =  1.68241005847144 \n",
      "Iterasjon  65  L =  1.6810340779264252 \n",
      "Iterasjon  66  L =  1.6813844143603027 \n",
      "Iterasjon  67  L =  1.6786543294157152 \n",
      "Iterasjon  68  L =  1.6789140411247427 \n",
      "Iterasjon  69  L =  1.6770757464516708 \n",
      "Iterasjon  70  L =  1.6767524585323108 \n",
      "Iterasjon  71  L =  1.6754715968755518 \n",
      "Iterasjon  72  L =  1.6745956803043904 \n",
      "Iterasjon  73  L =  1.673633014564508 \n",
      "Iterasjon  74  L =  1.6737625031318395 \n",
      "Iterasjon  75  L =  1.6718732092645368 \n",
      "Iterasjon  76  L =  1.6714333153865242 \n",
      "Iterasjon  77  L =  1.6692430390293758 \n",
      "Iterasjon  78  L =  1.6700125312597733 \n",
      "Iterasjon  79  L =  1.668262401653427 \n",
      "Iterasjon  80  L =  1.6679754343908892 \n",
      "Iterasjon  81  L =  1.6670093938656558 \n",
      "Iterasjon  82  L =  1.6680710307977638 \n",
      "Iterasjon  83  L =  1.665637762767536 \n",
      "Iterasjon  84  L =  1.6655537536254623 \n",
      "Iterasjon  85  L =  1.6635145690452091 \n",
      "Iterasjon  86  L =  1.6635419047459648 \n",
      "Iterasjon  87  L =  1.6617249051956993 \n",
      "Iterasjon  88  L =  1.6610537623251234 \n",
      "Iterasjon  89  L =  1.6605329625928247 \n",
      "Iterasjon  90  L =  1.6594208428279438 \n",
      "Iterasjon  91  L =  1.6585988136492997 \n",
      "Iterasjon  92  L =  1.6570306461002824 \n",
      "Iterasjon  93  L =  1.6562533191407849 \n",
      "Iterasjon  94  L =  1.6552964645584576 \n",
      "Iterasjon  95  L =  1.6545211519253087 \n",
      "Iterasjon  96  L =  1.6530645007740712 \n",
      "Iterasjon  97  L =  1.65342585200572 \n",
      "Iterasjon  98  L =  1.6537615638490548 \n",
      "Iterasjon  99  L =  1.6515607576372702 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xs = data['x_train']\n",
    "ys = data['y_train']\n",
    "\n",
    "n_batches = xs.shape[0]\n",
    "n_iters = 100\n",
    "step_size = 0.01\n",
    "\n",
    "#treningsløkke tilsvarende algoritme 4 (med gradient descent)\n",
    "for j in range(n_iters):\n",
    "    losses = []\n",
    "    for i in range(n_batches):\n",
    "        x = xs[i]\n",
    "        y = ys[i]\n",
    "\n",
    "        X = onehot(x,m)\n",
    "        Z = nn.forward(X)\n",
    "\n",
    "        losses.append(loss.forward(Z,y))\n",
    "        dLdZ = loss.backward()\n",
    "        nn.backward(dLdZ)\n",
    "        nn.step_adam(step_size, j + 1)\n",
    "    mean_loss = np.mean(losses)\n",
    "    print(\"Iterasjon \", str(j), \" L = \",mean_loss, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etter trening ser vi at W_2 er gitt ved\n",
    "\n",
    "W_2_post_opt = nn.layers[2].l2.params['w']['w'].copy()\n",
    "print(W_2_post_opt[0,0])\n",
    "\n",
    "#dersom differansen er større enn null har dette parameteret endret seg etter \n",
    "#gradient descent\n",
    "print(W_2_post_opt[0,0] - W_2_pre_opt[0,0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

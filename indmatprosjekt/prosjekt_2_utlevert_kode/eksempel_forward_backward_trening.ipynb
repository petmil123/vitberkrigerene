{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_addition\n",
    "\n",
    "#dimensjoner og størrelser til x og y\n",
    "n_digits = 2\n",
    "n_max = 3*n_digits\n",
    "m = 10\n",
    "\n",
    "#definerer størrelsen på parametermatrisene\n",
    "d = 15\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "#henter treningsdata\n",
    "data = get_train_test_addition(n_digits,samples_per_batch=100,n_batches_train=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import onehot\n",
    "\n",
    "x = data['x_train'][0]\n",
    "X = onehot(x,m)\n",
    "y = data['y_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 100, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['x_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import LinearLayer,EmbedPosition,Attention,Softmax,CrossEntropy,FeedForward\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vi ser at dLdW_2 tilhørende ff1 (først finner vi det lineære laget l2\n",
    "#så nøkkel 'w' i params dict-en, så nøkkel 'd' for å finne den deriverte)\n",
    "#nå er denne bare en nullmatrise \n",
    "ff1.l2.params['w']['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"manuelt\" forward pass (tilsvarende algoritme 1)\n",
    "z0 = embed.forward(X)\n",
    "z1 = att1.forward(z0)\n",
    "z2 = ff1.forward(z1)\n",
    "z = un_embed.forward(z2)\n",
    "Z = softmax.forward(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.304645857015904\n"
     ]
    }
   ],
   "source": [
    "#evaluerer objektfunksjonen\n",
    "L = loss.forward(Z,y)\n",
    "print(L)\n",
    "\n",
    "#finner den deriverte av objektfunksjonen mhp Z\n",
    "dLdZ = loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"manuelt\" backward pass (tilsvarende algoritme 2)\n",
    "dLdz = softmax.backward(dLdZ)\n",
    "dLdz2 = un_embed.backward(dLdz)\n",
    "dLdz1 = ff1.backward(dLdz2)\n",
    "dLdz0 = att1.backward(dLdz1)\n",
    "embed.backward(dLdz0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.11316356e-05,  1.90495525e-04, -4.53043131e-05,\n",
       "         1.04102072e-04, -4.20189329e-05, -1.57720936e-05,\n",
       "        -9.42090136e-05,  2.11193267e-05, -2.13131487e-04,\n",
       "         9.53218341e-05,  6.31796020e-06, -3.30341854e-05,\n",
       "        -6.48944261e-05,  6.93939535e-07,  8.99382560e-05,\n",
       "        -9.96417970e-05, -1.05875476e-05,  9.92815656e-05,\n",
       "         4.80592310e-05, -1.33901685e-04],\n",
       "       [ 4.36927303e-04,  7.05000577e-04,  2.06093563e-04,\n",
       "         1.71735628e-05,  5.06520510e-05,  3.63758179e-04,\n",
       "         1.38025295e-04, -6.18198169e-05,  4.62118627e-04,\n",
       "         7.70310270e-04, -1.03197190e-04,  1.55790681e-05,\n",
       "         3.34174859e-05, -9.60224903e-06,  2.72935228e-04,\n",
       "         3.95620957e-04,  2.19579983e-04,  2.99569717e-04,\n",
       "         6.58022094e-04,  3.64209057e-04],\n",
       "       [-4.19543271e-04, -5.94562079e-04,  1.24432226e-04,\n",
       "         5.21030661e-07,  9.49472893e-05,  1.40615544e-04,\n",
       "         3.98729384e-04, -9.90411940e-05,  4.11647503e-04,\n",
       "        -3.14338740e-04, -4.25877213e-05,  6.29199614e-06,\n",
       "        -1.17022432e-04, -2.81571342e-05,  1.77921807e-05,\n",
       "         9.92837478e-05,  7.85333021e-05, -2.05688535e-04,\n",
       "        -8.97473948e-05,  3.55877258e-04],\n",
       "       [-8.43495596e-05, -3.42197789e-04, -3.48354562e-05,\n",
       "         6.40853108e-05,  2.15281829e-05, -1.65151819e-04,\n",
       "         7.50164456e-05,  7.44433531e-05, -7.77667133e-05,\n",
       "        -6.16178154e-04,  1.04679911e-04,  2.70852553e-05,\n",
       "         1.95983577e-05,  1.69050069e-05, -8.72213761e-05,\n",
       "        -2.08413140e-04, -1.57344502e-05, -2.20163533e-04,\n",
       "        -3.10259479e-04, -1.54551061e-04],\n",
       "       [ 5.21962687e-04,  5.34560179e-04,  8.11094899e-04,\n",
       "         4.56803908e-04,  3.38355866e-04,  5.91508846e-04,\n",
       "         1.16234929e-03,  1.91511549e-04,  1.52154148e-03,\n",
       "         1.79763271e-05,  2.89248596e-04,  8.27222625e-05,\n",
       "         5.55186942e-05, -4.84579382e-06,  5.50682325e-04,\n",
       "         4.24256377e-04,  6.80281581e-04,  1.93772143e-04,\n",
       "         1.07247831e-03,  1.28367817e-03],\n",
       "       [ 4.05799097e-04,  4.06105426e-04,  5.64365655e-04,\n",
       "         2.32309755e-04,  2.40250755e-04,  4.96323820e-04,\n",
       "         8.11926599e-04,  5.97987434e-05,  1.26029923e-03,\n",
       "         3.27504344e-04,  1.12414149e-04,  8.45113219e-05,\n",
       "         1.69442195e-04, -5.77899361e-06,  3.57689590e-04,\n",
       "         4.72296373e-04,  4.79455307e-04,  1.41299221e-04,\n",
       "         7.97576279e-04,  1.00334422e-03],\n",
       "       [-1.76405340e-04,  1.16579924e-04, -1.46910145e-04,\n",
       "        -1.77744488e-04, -8.42233573e-05, -6.87930931e-06,\n",
       "        -2.65563004e-04, -1.29760042e-05, -2.14987888e-04,\n",
       "         4.71081300e-04, -7.59510102e-05, -4.57034556e-05,\n",
       "        -3.19749157e-05, -1.43166571e-05, -2.26756317e-04,\n",
       "         7.58790962e-05, -1.36944762e-04, -2.96141396e-05,\n",
       "         8.00989707e-05, -2.63929133e-05],\n",
       "       [ 5.01879335e-04,  4.42079397e-04,  4.30852238e-04,\n",
       "         2.45274477e-04,  1.94262702e-04,  3.26102287e-04,\n",
       "         5.34402359e-04,  1.31301538e-04,  7.92676290e-04,\n",
       "        -4.25154279e-05,  1.85147697e-04,  3.08511209e-05,\n",
       "        -6.94479932e-06,  7.15175874e-06,  2.64049491e-04,\n",
       "         2.77340906e-04,  3.39596095e-04,  1.96116831e-04,\n",
       "         5.33578135e-04,  5.76729969e-04],\n",
       "       [ 1.38592832e-04,  1.43862950e-04,  1.31377827e-05,\n",
       "        -2.55287165e-04, -9.46244232e-06, -1.25742204e-04,\n",
       "        -2.25213206e-04,  4.30464459e-05, -1.96499573e-04,\n",
       "        -1.71816657e-04,  3.52437857e-05,  2.95859075e-05,\n",
       "        -1.55065024e-05,  1.05706336e-05, -9.20405599e-05,\n",
       "         7.31088900e-06, -5.60457148e-05,  8.60097464e-05,\n",
       "         4.44979751e-05, -1.42117450e-04],\n",
       "       [ 2.98282983e-04,  3.15926890e-04,  2.82465027e-04,\n",
       "         7.98247094e-05,  1.09118417e-04,  7.09029128e-05,\n",
       "         2.81283237e-04,  1.39753180e-04,  3.95180694e-04,\n",
       "        -7.54087333e-05,  1.79221179e-04,  2.58483686e-05,\n",
       "         5.25392797e-05,  1.48212445e-05,  1.08844989e-04,\n",
       "         9.15146244e-05,  1.97594753e-04,  1.23098161e-04,\n",
       "         3.71336414e-04,  3.48795324e-04],\n",
       "       [ 2.59560776e-04,  2.00381843e-04,  9.46733030e-05,\n",
       "         2.04297055e-04,  1.89240969e-05,  7.59773765e-05,\n",
       "         7.75384744e-05,  9.01249033e-05,  1.15924464e-04,\n",
       "         3.05490640e-06,  9.22713493e-05,  6.68655154e-05,\n",
       "         1.60401650e-04,  5.62859148e-06,  1.78342351e-04,\n",
       "        -4.06743317e-05,  8.64136158e-05,  1.11225870e-04,\n",
       "         1.12882364e-04,  8.67337213e-05],\n",
       "       [-5.57953853e-04, -6.95780146e-04, -2.61898277e-04,\n",
       "        -1.36118312e-04, -9.44604478e-05, -2.85048669e-04,\n",
       "        -3.08156348e-04,  1.04671189e-05, -6.45664318e-04,\n",
       "        -6.59684491e-04,  1.60743469e-05, -1.91206976e-05,\n",
       "        -1.68583965e-04, -1.54595779e-05, -2.80358747e-04,\n",
       "        -3.43127607e-04, -2.91040913e-04, -2.49431039e-04,\n",
       "        -6.01183648e-04, -4.08092021e-04],\n",
       "       [-2.17138810e-04, -2.14397054e-04, -2.83558643e-05,\n",
       "         2.42030077e-04, -2.07937487e-05, -9.81587272e-05,\n",
       "         6.38094782e-05,  6.00556206e-05, -1.74603251e-04,\n",
       "        -1.62597371e-04,  1.04861914e-04, -5.76939598e-05,\n",
       "        -5.54513311e-05, -4.54547769e-06,  4.31707824e-05,\n",
       "        -2.50225723e-04, -5.37010801e-05,  1.35109947e-05,\n",
       "        -1.87575178e-04, -2.22762518e-05],\n",
       "       [-4.24613929e-06, -5.00333446e-05, -5.89646127e-05,\n",
       "        -7.96404646e-05,  9.38187741e-06,  3.81778387e-05,\n",
       "         3.30123207e-05, -8.38281334e-05,  9.59519888e-05,\n",
       "         3.11329161e-05, -1.05300932e-04, -1.12420084e-05,\n",
       "        -3.98275985e-05,  4.53632086e-06, -6.56484801e-05,\n",
       "         1.06058826e-04,  2.30343606e-05, -1.42086793e-04,\n",
       "        -4.31999170e-05, -6.61602409e-05],\n",
       "       [-4.16961201e-04, -5.08860608e-04, -1.42257166e-04,\n",
       "        -5.58910550e-05, -6.20310982e-05, -2.51801794e-04,\n",
       "        -1.06608754e-04, -4.61417217e-05, -5.49388378e-04,\n",
       "        -6.94539011e-04, -2.77170695e-05, -1.11821914e-05,\n",
       "        -2.18454606e-04, -6.92114026e-06,  2.67774394e-05,\n",
       "        -3.76326080e-04, -9.79673521e-05, -1.78761500e-04,\n",
       "        -3.55100356e-04, -3.83114454e-04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#etter backward pass har dLdW_2 fått verdier\n",
    "ff1.l2.params['w']['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import NeuralNetwork\n",
    "\n",
    "#vi kan samle lagene i en liste som vi bruker for å\n",
    "#initialisere et nevralt nettverk der vi kan bruke forward() og backward() \n",
    "#for å oppnå det samme som vi gjorde manuelt over\n",
    "\n",
    "layers = [embed,att1,ff1,un_embed,softmax]\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.304645857015904\n"
     ]
    }
   ],
   "source": [
    "#forward pass tilsvarende algoritme 1\n",
    "Z = nn.forward(X)\n",
    "\n",
    "#beregner loss med CrossEntropy\n",
    "L = loss.forward(Z,y)\n",
    "print(L)\n",
    "\n",
    "#backward pass tilsvarende algoritme 2\n",
    "dLdZ = loss.backward()\n",
    "nn.backward(dLdZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005562762195827531\n"
     ]
    }
   ],
   "source": [
    "#før optimering er W_2[0,0] gitt ved\n",
    "#ff1 er det tredje laget i layers-listen (derav layers[2])\n",
    "W_2_pre_opt = nn.layers[2].l2.params['w']['w'].copy()\n",
    "print(W_2_pre_opt[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "xs = data['x_train']\n",
    "ys = data['y_train']\n",
    "\n",
    "n_batches = xs.shape[0]\n",
    "n_iters = 100\n",
    "step_size = 0.1\n",
    "\n",
    "#treningsløkke tilsvarende algoritme 4 (med gradient descent)\n",
    "for j in range(n_iters):\n",
    "    losses = []\n",
    "    for i in range(n_batches):\n",
    "        x = xs[i]\n",
    "        y = ys[i]\n",
    "\n",
    "        X = onehot(x,m)\n",
    "        Z = nn.forward(X)\n",
    "\n",
    "        losses.append(loss.forward(Z,y))\n",
    "        dLdZ = loss.backward()\n",
    "        nn.backward(dLdZ)\n",
    "        nn.step_gd(step_size)\n",
    "    mean_loss = np.mean(losses)\n",
    "    print(\"Iterasjon \", str(j), \" L = \",mean_loss, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  1.4519663330133152 \n",
      "Iterasjon  1  L =  1.4460636443884443 \n",
      "Iterasjon  2  L =  1.44430994053198 \n",
      "Iterasjon  3  L =  1.4411505061272907 \n",
      "Iterasjon  4  L =  1.441025520786782 \n",
      "Iterasjon  5  L =  1.4403246932519718 \n",
      "Iterasjon  6  L =  1.4390622740764742 \n",
      "Iterasjon  7  L =  1.4388454925997378 \n",
      "Iterasjon  8  L =  1.4395530676674646 \n",
      "Iterasjon  9  L =  1.4384259540092477 \n",
      "Iterasjon  10  L =  1.4375002983931828 \n",
      "Iterasjon  11  L =  1.4367107533879016 \n",
      "Iterasjon  12  L =  1.437307199584412 \n",
      "Iterasjon  13  L =  1.4370136180771453 \n",
      "Iterasjon  14  L =  1.4362943903951881 \n",
      "Iterasjon  15  L =  1.4361025132583822 \n",
      "Iterasjon  16  L =  1.435953271028029 \n",
      "Iterasjon  17  L =  1.437020702447779 \n",
      "Iterasjon  18  L =  1.4364467168050614 \n",
      "Iterasjon  19  L =  1.4352016427240224 \n",
      "Iterasjon  20  L =  1.4350948593513992 \n",
      "Iterasjon  21  L =  1.4352546165374547 \n",
      "Iterasjon  22  L =  1.434329171840305 \n",
      "Iterasjon  23  L =  1.4347436677931067 \n",
      "Iterasjon  24  L =  1.436160754494114 \n",
      "Iterasjon  25  L =  1.4354320029346286 \n",
      "Iterasjon  26  L =  1.435359351921295 \n",
      "Iterasjon  27  L =  1.4359515228702546 \n",
      "Iterasjon  28  L =  1.4363371493898027 \n",
      "Iterasjon  29  L =  1.4358196071648317 \n",
      "Iterasjon  30  L =  1.4358356424390821 \n",
      "Iterasjon  31  L =  1.4358099959700388 \n",
      "Iterasjon  32  L =  1.4366424159942608 \n",
      "Iterasjon  33  L =  1.4347359971999112 \n",
      "Iterasjon  34  L =  1.4358263657096877 \n",
      "Iterasjon  35  L =  1.4336173461758288 \n",
      "Iterasjon  36  L =  1.4342796379617235 \n",
      "Iterasjon  37  L =  1.4337221828684281 \n",
      "Iterasjon  38  L =  1.4354741262868433 \n",
      "Iterasjon  39  L =  1.4337362883202909 \n",
      "Iterasjon  40  L =  1.4344181354523755 \n",
      "Iterasjon  41  L =  1.433306143817194 \n",
      "Iterasjon  42  L =  1.4353560235370235 \n",
      "Iterasjon  43  L =  1.4345331950354154 \n",
      "Iterasjon  44  L =  1.4338515483858223 \n",
      "Iterasjon  45  L =  1.433878376369604 \n",
      "Iterasjon  46  L =  1.4358052260473375 \n",
      "Iterasjon  47  L =  1.432096188702564 \n",
      "Iterasjon  48  L =  1.4355194635872133 \n",
      "Iterasjon  49  L =  1.4339772614957778 \n",
      "Iterasjon  50  L =  1.433947764307511 \n",
      "Iterasjon  51  L =  1.4357489357268007 \n",
      "Iterasjon  52  L =  1.4344847449971718 \n",
      "Iterasjon  53  L =  1.4334454079426404 \n",
      "Iterasjon  54  L =  1.4355972071453589 \n",
      "Iterasjon  55  L =  1.4347232527628866 \n",
      "Iterasjon  56  L =  1.4341230177513495 \n",
      "Iterasjon  57  L =  1.4322173082783454 \n",
      "Iterasjon  58  L =  1.432529904384518 \n",
      "Iterasjon  59  L =  1.4330744693145683 \n",
      "Iterasjon  60  L =  1.4353022868558525 \n",
      "Iterasjon  61  L =  1.4381413675927206 \n",
      "Iterasjon  62  L =  1.4354329692222414 \n",
      "Iterasjon  63  L =  1.4343832040533424 \n",
      "Iterasjon  64  L =  1.433987612405197 \n",
      "Iterasjon  65  L =  1.4321734641029047 \n",
      "Iterasjon  66  L =  1.4331051956029748 \n",
      "Iterasjon  67  L =  1.4334515225500075 \n",
      "Iterasjon  68  L =  1.438003591230995 \n",
      "Iterasjon  69  L =  1.4370178376787428 \n",
      "Iterasjon  70  L =  1.4337255519599985 \n",
      "Iterasjon  71  L =  1.4321888195919628 \n",
      "Iterasjon  72  L =  1.4347731631435425 \n",
      "Iterasjon  73  L =  1.4326911276567917 \n",
      "Iterasjon  74  L =  1.433471201932373 \n",
      "Iterasjon  75  L =  1.4369748958122814 \n",
      "Iterasjon  76  L =  1.4358184299708077 \n",
      "Iterasjon  77  L =  1.4380751773868028 \n",
      "Iterasjon  78  L =  1.4372988387017926 \n",
      "Iterasjon  79  L =  1.435624830712671 \n",
      "Iterasjon  80  L =  1.4362797671268168 \n",
      "Iterasjon  81  L =  1.4338209997544595 \n",
      "Iterasjon  82  L =  1.4318450970663785 \n",
      "Iterasjon  83  L =  1.4326147359450476 \n",
      "Iterasjon  84  L =  1.4334805700246074 \n",
      "Iterasjon  85  L =  1.4340593358124463 \n",
      "Iterasjon  86  L =  1.4343500300502212 \n",
      "Iterasjon  87  L =  1.4325567441226432 \n",
      "Iterasjon  88  L =  1.4358081727661502 \n",
      "Iterasjon  89  L =  1.4335353896672118 \n",
      "Iterasjon  90  L =  1.436109568151859 \n",
      "Iterasjon  91  L =  1.448536457093803 \n",
      "Iterasjon  92  L =  1.4457413828286656 \n",
      "Iterasjon  93  L =  1.4505919186827394 \n",
      "Iterasjon  94  L =  1.4506978647107356 \n",
      "Iterasjon  95  L =  1.4573591316886259 \n",
      "Iterasjon  96  L =  1.4515672371645874 \n",
      "Iterasjon  97  L =  1.4431964178024113 \n",
      "Iterasjon  98  L =  1.4472235959743927 \n",
      "Iterasjon  99  L =  1.4353411046125921 \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xs = data['x_train']\n",
    "ys = data['y_train']\n",
    "\n",
    "n_batches = xs.shape[0]\n",
    "n_iters = 100\n",
    "step_size = 0.02\n",
    "\n",
    "#treningsløkke tilsvarende algoritme 4 (med gradient descent)\n",
    "for j in range(n_iters):\n",
    "    losses = []\n",
    "    for i in range(n_batches):\n",
    "        x = xs[i]\n",
    "        y = ys[i]\n",
    "\n",
    "        X = onehot(x,m)\n",
    "        Z = nn.forward(X)\n",
    "\n",
    "        losses.append(loss.forward(Z,y))\n",
    "        dLdZ = loss.backward()\n",
    "        nn.backward(dLdZ)\n",
    "        nn.step_adam(step_size, j + 1)\n",
    "    mean_loss = np.mean(losses)\n",
    "    print(\"Iterasjon \", str(j), \" L = \",mean_loss, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etter trening ser vi at W_2 er gitt ved\n",
    "\n",
    "W_2_post_opt = nn.layers[2].l2.params['w']['w'].copy()\n",
    "print(W_2_post_opt[0,0])\n",
    "\n",
    "#dersom differansen er større enn null har dette parameteret endret seg etter \n",
    "#gradient descent\n",
    "print(W_2_post_opt[0,0] - W_2_pre_opt[0,0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

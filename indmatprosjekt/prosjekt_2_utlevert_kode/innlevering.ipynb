{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The transformer model for sequence prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is all about *learning* useful *functions* from big *datasets*. These useful functions are called nevral networks, and are put together from smaller functions with parameters that are decided through optimization. In opposition to conventional programming, where we tell the computer what to do, nevral networks learns from observational data and figure out its own solution to the given problem. Here we will implement the transformer model, one of the main components in big languagemodels like *ChatGPT*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.0** Structure of the datasets and the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Let          $a = 15$, $b = 7$, $c = 47$, $d = 152$\n",
    "\n",
    "then we have   $[1, 5, 7, 4, 7, 1, 5]$, $y =[1, 5, 2]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Let   \n",
    "\n",
    "$x^{(0)} = [1, 5, 7, 4, 7]$\n",
    "\n",
    "$x^{(1)} = [1, 5, 7, 4, 7, \\hat{z_4}]$\n",
    "\n",
    "$x^{(2)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}]$\n",
    "\n",
    "$x^{(3)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}, \\hat{z_6}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(0)}}, \\hat{z_1^{(0)}}, \\hat{z_2^{(0)}}, \\hat{z_3^{(0)}}, \\hat{z_4^{(0)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(1)}}, \\hat{z_1^{(1)}}, \\hat{z_2^{(1)}}, \\hat{z_3^{(1)}}, \\hat{z_4^{(1)}}, \\hat{z_5^{(1)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(2)}}, \\hat{z_1^{(2)}}, \\hat{z_2^{(2)}}, \\hat{z_3^{(2)}}, \\hat{z_4^{(2)}}, \\hat{z_5^{(2)}}, \\hat{z_6^{(2)}}]$\n",
    "\n",
    "If the optimization is good, the result should be:\n",
    "\n",
    "$\\hat{z_4^{(0)}} = 1, \\hat{z_5^{(1)}} = 5$ og $\\hat{z_6^{(2)}} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)**\n",
    "\n",
    "For the object function to be $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$, the probability distribution must be given by:\n",
    "\n",
    "$\\hat{Y} = onehot(y) = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "In this case $\\hat{y}$ will be given by:\n",
    "\n",
    "$\\hat{y} := argmax(\\hat{Y}) = y$\n",
    "\n",
    "Then, $\\mathcal(L) = 0$ will be fulfilled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)**\n",
    "\n",
    "The number of parameters is given by:\n",
    "\n",
    "$d(2m + n_{max} + L(4k + 2p))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)**\n",
    "\n",
    "$X = onehot(x) = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}, z_0 = W_Ex + [W_P]_{0:n} = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Z = softmax(\\begin{bmatrix}\n",
    "1\n",
    "\\alpha\n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "\\frac{e^1}{e^1+1^{\\alpha}} \\\\\n",
    "\\frac{e^{\\alpha}}{e^1+e^{\\alpha}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\hat{z} = 1 \\Rightarrow \\alpha > 1$ (when $\\alpha=1$, undefined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.0** Implementing the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** \n",
    "\n",
    "1) If the type of layer is identified as `LinearLayer` or `Attention`, `NeuralNetwork` will inherit `step_gd` from the `Layer` class. \n",
    "\n",
    "2) If the type of layer is identified as `EmbedPosition`, `NeuralNetwork` will inherit `step_gd` from the `EmbedPosition` class. \n",
    "\n",
    "3) If the type of layer is identified as `FeedForward`, `NeuralNetwork` will inherit `step_gd` from the `FeedForward` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from layers import *\n",
    "from training import trainModel\n",
    "import numpy as np\n",
    "from data_generators import get_train_test_addition, get_train_test_sorting\n",
    "from training import *\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5\n",
    "m = 2\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2*r-1\n",
    "sigma = Relu\n",
    "\n",
    "data = get_train_test_sorting(r,m,batchSize, batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "att_ffd_list = []\n",
    "for layer in range(L):\n",
    "    att = Attention(d,k)\n",
    "    ff = FeedForward(d,p)\n",
    "    att_ffd_list.append(att)\n",
    "    att_ffd_list.append(ff)\n",
    "\n",
    "layers = [embed] + att_ffd_list + [un_embed] + [softmax]\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  0.6947286792957972  gradient =  0.031177056119975666\n",
      "Iterasjon  1  L =  0.6712480552744214  gradient =  0.03051751538866585\n",
      "Iterasjon  2  L =  0.6466883614756098  gradient =  0.029879170541681176\n",
      "Iterasjon  3  L =  0.6193256280399076  gradient =  0.029289650816542237\n",
      "Iterasjon  4  L =  0.5888236306397688  gradient =  0.02880281075473611\n",
      "Iterasjon  5  L =  0.5553892689179792  gradient =  0.028516411899962173\n",
      "Iterasjon  6  L =  0.5201036799030496  gradient =  0.028538085091042057\n",
      "Iterasjon  7  L =  0.4842983378287739  gradient =  0.028888411308381826\n",
      "Iterasjon  8  L =  0.44894555425566784  gradient =  0.02938609008099899\n",
      "Iterasjon  9  L =  0.41511625038587907  gradient =  0.02996841612987262\n",
      "Iterasjon  10  L =  0.38493646627447686  gradient =  0.030837794857565732\n",
      "Iterasjon  11  L =  0.3603709734773802  gradient =  0.03223410869451063\n",
      "Iterasjon  12  L =  0.3413612576218047  gradient =  0.03390053535572599\n",
      "Iterasjon  13  L =  0.32720123211630875  gradient =  0.03510829441669503\n",
      "Iterasjon  14  L =  0.31672022473263317  gradient =  0.03546047121717634\n",
      "Iterasjon  15  L =  0.30845066047564457  gradient =  0.03515050959231807\n",
      "Iterasjon  16  L =  0.3014048683391775  gradient =  0.034460031148266235\n",
      "Iterasjon  17  L =  0.29503705487124954  gradient =  0.033582465147396094\n",
      "Iterasjon  18  L =  0.2889677970370751  gradient =  0.03265730694000009\n",
      "Iterasjon  19  L =  0.2830150339951095  gradient =  0.03168350703626621\n",
      "Iterasjon  20  L =  0.2769050789532003  gradient =  0.030695970934429034\n",
      "Iterasjon  21  L =  0.2701382778932252  gradient =  0.029287098902067873\n",
      "Iterasjon  22  L =  0.26234398297709505  gradient =  0.02803747954284173\n",
      "Iterasjon  23  L =  0.25378113964704335  gradient =  0.026822210766548855\n",
      "Iterasjon  24  L =  0.24394694537825531  gradient =  0.025608544378369458\n",
      "Iterasjon  25  L =  0.23143615101649093  gradient =  0.023886456837864074\n",
      "Iterasjon  26  L =  0.2164686546746682  gradient =  0.02282165863908587\n",
      "Iterasjon  27  L =  0.2006359862383266  gradient =  0.021955396536907194\n",
      "Iterasjon  28  L =  0.1826303092944716  gradient =  0.021108787780163077\n",
      "Iterasjon  29  L =  0.16232471011152177  gradient =  0.02011257622157437\n",
      "Iterasjon  30  L =  0.13959679074627235  gradient =  0.019073204863511477\n",
      "Iterasjon  31  L =  0.11332640573601607  gradient =  0.018094588965405826\n",
      "Iterasjon  32  L =  0.08631310943085199  gradient =  0.01735831051319362\n",
      "Iterasjon  33  L =  0.06203449252782852  gradient =  0.01676885190501149\n",
      "Iterasjon  34  L =  0.04282420819633773  gradient =  0.016393003192124427\n",
      "Iterasjon  35  L =  0.029026757564248884  gradient =  0.016152378843824977\n",
      "Iterasjon  36  L =  0.020009040464786118  gradient =  0.01601436193115285\n",
      "Iterasjon  37  L =  0.014531305599140242  gradient =  0.01593815808377263\n",
      "Iterasjon  38  L =  0.011337431745212494  gradient =  0.015895777022503093\n",
      "Iterasjon  39  L =  0.00933315629217187  gradient =  0.015869477381933426\n",
      "Iterasjon  40  L =  0.007921943700623517  gradient =  0.015846042294921116\n",
      "Iterasjon  41  L =  0.0069083539559189455  gradient =  0.01582501224426606\n",
      "Iterasjon  42  L =  0.006181449268799091  gradient =  0.015812190797178573\n",
      "Iterasjon  43  L =  0.005652950340363723  gradient =  0.015804532672171297\n",
      "Iterasjon  44  L =  0.005222041753195262  gradient =  0.01579703320973761\n",
      "Iterasjon  45  L =  0.004854709728031993  gradient =  0.01579073332056864\n",
      "Iterasjon  46  L =  0.004550861172569658  gradient =  0.015785515942499818\n",
      "Iterasjon  47  L =  0.004283276650138939  gradient =  0.015780743001272638\n",
      "Iterasjon  48  L =  0.0040488528154637455  gradient =  0.015776795536712775\n",
      "Iterasjon  49  L =  0.0038393017066664834  gradient =  0.015773050718456673\n",
      "Iterasjon  50  L =  0.0036514136571610756  gradient =  0.015769791333248545\n",
      "Iterasjon  51  L =  0.0034806356781758294  gradient =  0.015766776356768614\n",
      "Iterasjon  52  L =  0.003324993753851549  gradient =  0.01576407078355775\n",
      "Iterasjon  53  L =  0.0031814034415334575  gradient =  0.01576155214461425\n",
      "Iterasjon  54  L =  0.0030494440984918784  gradient =  0.01575927378076357\n",
      "Iterasjon  55  L =  0.0029262173755350983  gradient =  0.015757117857786454\n",
      "Iterasjon  56  L =  0.0028121704024885097  gradient =  0.015755136671789356\n",
      "Iterasjon  57  L =  0.002705216583458751  gradient =  0.015753306342805386\n",
      "Iterasjon  58  L =  0.0026048013770569095  gradient =  0.015751569849808654\n",
      "Iterasjon  59  L =  0.002510509667163054  gradient =  0.01574999164962066\n",
      "Iterasjon  60  L =  0.0024211568033642135  gradient =  0.015748474035068738\n",
      "Iterasjon  61  L =  0.0023366834762806954  gradient =  0.015747041323929078\n",
      "Iterasjon  62  L =  0.0022564056376903714  gradient =  0.01574571657598477\n",
      "Iterasjon  63  L =  0.0021801574136345193  gradient =  0.01574445545192487\n",
      "Iterasjon  64  L =  0.0021074804496707517  gradient =  0.01574325130090008\n",
      "Iterasjon  65  L =  0.002038172690855805  gradient =  0.015742113125907096\n",
      "Iterasjon  66  L =  0.00197190147035699  gradient =  0.015741024823595635\n",
      "Iterasjon  67  L =  0.001908371269209997  gradient =  0.015740000374369834\n",
      "Iterasjon  68  L =  0.0018473171161986642  gradient =  0.01573902016178418\n",
      "Iterasjon  69  L =  0.0017886630842397402  gradient =  0.015738085444435054\n",
      "Iterasjon  70  L =  0.0017322202282789739  gradient =  0.01573719247288594\n",
      "Iterasjon  71  L =  0.0016778303945006617  gradient =  0.015736335763128956\n",
      "Iterasjon  72  L =  0.001625386782058517  gradient =  0.015735511092718067\n",
      "Iterasjon  73  L =  0.0015747543522804837  gradient =  0.015734723408214864\n",
      "Iterasjon  74  L =  0.0015258094972926783  gradient =  0.01573396955095819\n",
      "Iterasjon  75  L =  0.0014784574325551322  gradient =  0.015733244476615497\n",
      "Iterasjon  76  L =  0.001432761873558694  gradient =  0.015732555880494176\n",
      "Iterasjon  77  L =  0.0013885494945444566  gradient =  0.015731890407446422\n",
      "Iterasjon  78  L =  0.0013457931074445152  gradient =  0.01573124720304216\n",
      "Iterasjon  79  L =  0.001304312090062857  gradient =  0.015730631449665515\n",
      "Iterasjon  80  L =  0.0012640817022575865  gradient =  0.01573003457940136\n",
      "Iterasjon  81  L =  0.001225046489113945  gradient =  0.015729459120740127\n",
      "Iterasjon  82  L =  0.0011871246586834705  gradient =  0.01572890560281358\n",
      "Iterasjon  83  L =  0.0011502804188781484  gradient =  0.015728370666564902\n",
      "Iterasjon  84  L =  0.001114500383590893  gradient =  0.01572785544543427\n",
      "Iterasjon  85  L =  0.0010797489296483441  gradient =  0.015727358653935573\n",
      "Iterasjon  86  L =  0.0010459764807400178  gradient =  0.015726877606817905\n",
      "Iterasjon  87  L =  0.0010134574542024059  gradient =  0.015726462895696583\n",
      "Iterasjon  88  L =  0.0009811602629329137  gradient =  0.0157259601738062\n",
      "Iterasjon  89  L =  0.0009507805195425798  gradient =  0.01572556732559258\n",
      "Iterasjon  90  L =  0.0009201380672630644  gradient =  0.015725121291367807\n",
      "Iterasjon  91  L =  0.0008916471530343159  gradient =  0.015724768510459935\n",
      "Iterasjon  92  L =  0.0008624710343134729  gradient =  0.015724330294060677\n",
      "Iterasjon  93  L =  0.0008352438050837593  gradient =  0.015723922045268048\n",
      "Iterasjon  94  L =  0.0008083019820627587  gradient =  0.015723591101875548\n",
      "Iterasjon  95  L =  0.0007826677586218359  gradient =  0.015723284335984525\n",
      "Iterasjon  96  L =  0.000756729276938599  gradient =  0.01572288501690807\n",
      "Iterasjon  97  L =  0.0007327052314492761  gradient =  0.015722571597813463\n",
      "Iterasjon  98  L =  0.0007084724979857461  gradient =  0.01572225097672817\n",
      "Iterasjon  99  L =  0.0006858040746234625  gradient =  0.015721972237726124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = trainModel(nn,data,100,loss, m, r, 0.001)\n",
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10f8f2b10>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "len(losses)\n",
    "plt.semilogy([i for i in range(len(losses))], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DO NOT RUN IF NOT NEW TRAINED MODEL\n",
    "# with open(\"sortingTrained_v1\", 'wb') as f:\n",
    "    # pickle.dump(nn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"savedObject\", 'rb') as f:\n",
    "     #nn2 = pickle.load(f)\n",
    "\n",
    "#type(nn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(250, 5)\n",
      "(250, 5)\n",
      "1\n",
      "(250, 6)\n",
      "(250, 6)\n",
      "2\n",
      "(250, 7)\n",
      "(250, 7)\n",
      "3\n",
      "(250, 8)\n",
      "(250, 8)\n",
      "4\n",
      "(250, 9)\n",
      "(250, 9)\n"
     ]
    }
   ],
   "source": [
    "with open(\"sortingTrained_v1\", \"rb\") as f:\n",
    "    nn = pickle.load(f)\n",
    "\n",
    "y_pred = predict(nn, data['x_test'], r, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 1. 1. 0.]\n",
      "  [1. 1. 1. 0. 1.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 1. 1. 1. 1.]\n",
      "  [1. 1. 0. 0. 0.]\n",
      "  [1. 1. 0. 1. 0.]]]\n",
      "\n",
      "[[[0. 0. 0. 1. 1.]\n",
      "  [0. 1. 1. 1. 1.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 1. 1. 1. 1.]\n",
      "  [0. 0. 0. 1. 1.]\n",
      "  [0. 0. 1. 1. 1.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print()\n",
    "print(data['y_test'])\n",
    "np.count_nonzero(np.count_nonzero(y_pred == data['y_test'], axis=2) == y_pred.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 7\n",
    "m = 5\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "iterations = 100\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2*r-1\n",
    "sigma = Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_train_test_sorting(r,m,batchSize, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed,att1,ff1,att2, ff2, un_embed,softmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  1.285825890802525  gradient =  0.37095785026214595\n",
      "Iterasjon  1  L =  0.824830356288792  gradient =  0.052003595374569826\n",
      "Iterasjon  2  L =  0.6876205951819652  gradient =  0.06067245364759717\n",
      "Iterasjon  3  L =  0.526436267960597  gradient =  0.17114650680997331\n",
      "Iterasjon  4  L =  0.370168057297904  gradient =  0.1498090849579385\n",
      "Iterasjon  5  L =  0.25379339080600805  gradient =  1.4931455508729572\n",
      "Iterasjon  6  L =  0.17254273285617097  gradient =  1.1662159768490414\n",
      "Iterasjon  7  L =  0.10505998145373656  gradient =  0.24460446711596023\n",
      "Iterasjon  8  L =  0.0644449093009009  gradient =  0.32330452342514054\n",
      "Iterasjon  9  L =  0.04062496965245094  gradient =  0.08923127003176944\n",
      "Iterasjon  10  L =  0.023948282219534104  gradient =  0.044129927246557295\n",
      "Iterasjon  11  L =  0.01630014070875887  gradient =  0.014553581357452348\n",
      "Iterasjon  12  L =  0.011362349148199728  gradient =  0.013129998931028912\n",
      "Iterasjon  13  L =  0.008177322684316824  gradient =  0.01297545823704062\n",
      "Iterasjon  14  L =  0.006179917224547303  gradient =  0.01294511607946368\n",
      "Iterasjon  15  L =  0.004958591154045242  gradient =  0.012930441318551616\n",
      "Iterasjon  16  L =  0.004170299341965078  gradient =  0.012921522607233275\n",
      "Iterasjon  17  L =  0.003578307823818383  gradient =  0.012915151420879888\n",
      "Iterasjon  18  L =  0.003154253708376196  gradient =  0.01291195049495328\n",
      "Iterasjon  19  L =  0.002814582925705727  gradient =  0.012910302102905129\n",
      "Iterasjon  20  L =  0.0025199894542232387  gradient =  0.012908229026970051\n",
      "Iterasjon  21  L =  0.0022460626779218147  gradient =  0.01290267001544309\n",
      "Iterasjon  22  L =  0.0019761789473965925  gradient =  0.01289315767061004\n",
      "Iterasjon  23  L =  0.0017504082743821862  gradient =  0.012889135116107748\n",
      "Iterasjon  24  L =  0.0015109460513325803  gradient =  0.01288849736246812\n",
      "Iterasjon  25  L =  0.001310016053038964  gradient =  0.012886525046183966\n",
      "Iterasjon  26  L =  0.001175847180649831  gradient =  0.012885074354665604\n",
      "Iterasjon  27  L =  0.0010868158257904546  gradient =  0.012884257924152995\n",
      "Iterasjon  28  L =  0.0010068944953846492  gradient =  0.012883725952944689\n",
      "Iterasjon  29  L =  0.000929272403157728  gradient =  0.01288284009316644\n",
      "Iterasjon  30  L =  0.0008784371034779555  gradient =  0.012882113978565871\n",
      "Iterasjon  31  L =  0.0008341197024458889  gradient =  0.012881378058068722\n",
      "Iterasjon  32  L =  0.0007918086234075209  gradient =  0.01288046245063105\n",
      "Iterasjon  33  L =  0.0007575904580487642  gradient =  0.012879635374537825\n",
      "Iterasjon  34  L =  0.0007224180195587313  gradient =  0.01287884274657836\n",
      "Iterasjon  35  L =  0.0006901963503548754  gradient =  0.012878159179213677\n",
      "Iterasjon  36  L =  0.0006565164818795828  gradient =  0.012877530030423344\n",
      "Iterasjon  37  L =  0.0006197213951177973  gradient =  0.012876974367172554\n",
      "Iterasjon  38  L =  0.0005807204072108682  gradient =  0.01287656457289105\n",
      "Iterasjon  39  L =  0.0005308492047245219  gradient =  0.012876224625863212\n",
      "Iterasjon  40  L =  0.0004727768652455184  gradient =  0.012876024319934325\n",
      "Iterasjon  41  L =  0.00041406164432485044  gradient =  0.012875746631970122\n",
      "Iterasjon  42  L =  0.00037340017808162515  gradient =  0.012875433003883717\n",
      "Iterasjon  43  L =  0.00035113203273628155  gradient =  0.01287524216547969\n",
      "Iterasjon  44  L =  0.0003400946440596517  gradient =  0.012875088828629\n",
      "Iterasjon  45  L =  0.00032899552233687655  gradient =  0.01287492631752679\n",
      "Iterasjon  46  L =  0.00031352816753991904  gradient =  0.012874766495441666\n",
      "Iterasjon  47  L =  0.000297876352456765  gradient =  0.012874641167999763\n",
      "Iterasjon  48  L =  0.0002838208523602382  gradient =  0.012874494623449723\n",
      "Iterasjon  49  L =  0.0002726193041399224  gradient =  0.012874388669221894\n",
      "Iterasjon  50  L =  0.00026146802864000015  gradient =  0.012874257406606554\n",
      "Iterasjon  51  L =  0.00025148804728045344  gradient =  0.012874165317072264\n",
      "Iterasjon  52  L =  0.00024059779188104918  gradient =  0.012874047696512932\n",
      "Iterasjon  53  L =  0.000231997143850513  gradient =  0.012873971235501118\n",
      "Iterasjon  54  L =  0.00022334979454125627  gradient =  0.012873876661240916\n",
      "Iterasjon  55  L =  0.00021439552876797663  gradient =  0.012873787025814057\n",
      "Iterasjon  56  L =  0.0002066291239738114  gradient =  0.012873714912793027\n",
      "Iterasjon  57  L =  0.000199124321973685  gradient =  0.01287363363693361\n",
      "Iterasjon  58  L =  0.00019208049637451453  gradient =  0.012873570060049681\n",
      "Iterasjon  59  L =  0.0001854318275483016  gradient =  0.012873502530840717\n",
      "Iterasjon  60  L =  0.00017877209009563386  gradient =  0.012873431065582416\n",
      "Iterasjon  61  L =  0.00017307920071348476  gradient =  0.012873376946779381\n",
      "Iterasjon  62  L =  0.0001674104546396106  gradient =  0.012873316757897027\n",
      "Iterasjon  63  L =  0.0001617956735458924  gradient =  0.012873264223223387\n",
      "Iterasjon  64  L =  0.00015683901381290395  gradient =  0.012873212470818254\n",
      "Iterasjon  65  L =  0.00015157607812924288  gradient =  0.012873152683440497\n",
      "Iterasjon  66  L =  0.00014720477175221995  gradient =  0.012873114745246484\n",
      "Iterasjon  67  L =  0.00014275579391802027  gradient =  0.012873065051039679\n",
      "Iterasjon  68  L =  0.00013799365888848613  gradient =  0.012873020146952428\n",
      "Iterasjon  69  L =  0.00013444432268663265  gradient =  0.012872983780884736\n",
      "Iterasjon  70  L =  0.00013043542979266994  gradient =  0.012872938393009736\n",
      "Iterasjon  71  L =  0.0001265117912424559  gradient =  0.012872904632363569\n",
      "Iterasjon  72  L =  0.00012277886242730127  gradient =  0.01287286549619875\n",
      "Iterasjon  73  L =  0.0001193296460980735  gradient =  0.012872830089759627\n",
      "Iterasjon  74  L =  0.00011637394444233878  gradient =  0.012872800536262408\n",
      "Iterasjon  75  L =  0.00011293712579446417  gradient =  0.012872764746501392\n",
      "Iterasjon  76  L =  0.00011005396356030862  gradient =  0.012872738403493575\n",
      "Iterasjon  77  L =  0.00010689650530350289  gradient =  0.012872703084287227\n",
      "Iterasjon  78  L =  0.00010468782370908927  gradient =  0.012872681222567892\n",
      "Iterasjon  79  L =  0.00010164964801474605  gradient =  0.012872648803496686\n",
      "Iterasjon  80  L =  9.867318396874945e-05  gradient =  0.012872624292060401\n",
      "Iterasjon  81  L =  9.565111844951307e-05  gradient =  0.012872596659317252\n",
      "Iterasjon  82  L =  9.291970288231185e-05  gradient =  0.012872569362256019\n",
      "Iterasjon  83  L =  9.057718736934402e-05  gradient =  0.012872548091271644\n",
      "Iterasjon  84  L =  8.792141632574586e-05  gradient =  0.01287252183899312\n",
      "Iterasjon  85  L =  8.550500788933021e-05  gradient =  0.012872500780371398\n",
      "Iterasjon  86  L =  8.322514150158893e-05  gradient =  0.012872478079515375\n",
      "Iterasjon  87  L =  8.143204839084912e-05  gradient =  0.012872462201113877\n",
      "Iterasjon  88  L =  7.90495848557264e-05  gradient =  0.012872439360335433\n",
      "Iterasjon  89  L =  7.684855980611679e-05  gradient =  0.012872419288077586\n",
      "Iterasjon  90  L =  7.524010064120383e-05  gradient =  0.012872402986161203\n",
      "Iterasjon  91  L =  7.329750314615237e-05  gradient =  0.012872383566163714\n",
      "Iterasjon  92  L =  7.132283349525862e-05  gradient =  0.012872366054298428\n",
      "Iterasjon  93  L =  6.996081384602265e-05  gradient =  0.012872352635845803\n",
      "Iterasjon  94  L =  6.814988273003942e-05  gradient =  0.012872334731129404\n",
      "Iterasjon  95  L =  6.646191883571366e-05  gradient =  0.012872320298420057\n",
      "Iterasjon  96  L =  6.499133362881372e-05  gradient =  0.012872304861108385\n",
      "Iterasjon  97  L =  6.358142941929044e-05  gradient =  0.01287229163227938\n",
      "Iterasjon  98  L =  6.201626331763679e-05  gradient =  0.01287227677664288\n",
      "Iterasjon  99  L =  6.061091814884089e-05  gradient =  0.012872262967194873\n"
     ]
    }
   ],
   "source": [
    "losses = trainModel(nn,data, iterations, loss, m, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "batches = 20\n",
    "d = 30\n",
    "k=20\n",
    "p=40\n",
    "L=3\n",
    "m=10\n",
    "n_max = 2*2 + 3\n",
    "\n",
    "data =get_train_test_addition(2,batch_size,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "layers.append(embed)\n",
    "for i in range(L):\n",
    "    att1 = Attention(d,k)\n",
    "    ff1 = FeedForward(d,p)\n",
    "\n",
    "    layers.append(att1)\n",
    "    layers.append(ff1)\n",
    "\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "layers.append(un_embed)\n",
    "softmax = Softmax()\n",
    "layers.append(softmax)\n",
    "\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (30,7) (30,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#her kjem da dimensjonsfeil må sjekkes ut\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VitenskapeligeBeregninger/indmatprosjekt/prosjekt_2_utlevert_kode/training.py:26\u001b[0m, in \u001b[0;36mtrainModel\u001b[0;34m(nn, data, iterations, loss, m, slice_number, step_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m         dLdZ \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m         nn\u001b[38;5;241m.\u001b[39mbackward(dLdZ)\n\u001b[0;32m---> 26\u001b[0m         \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterasjon \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(j), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m L = \u001b[39m\u001b[38;5;124m\"\u001b[39m,np\u001b[38;5;241m.\u001b[39mmean(losses[j,:]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m gradient = \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(dLdZ))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(losses, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/VitenskapeligeBeregninger/indmatprosjekt/prosjekt_2_utlevert_kode/neural_network.py:49\u001b[0m, in \u001b[0;36mNeuralNetwork.step_adam\u001b[0;34m(self, alpha, j)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m#Check if layer is of class a class that has parameters\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer,(LinearLayer,EmbedPosition,FeedForward,Attention)):\n\u001b[0;32m---> 49\u001b[0m         \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VitenskapeligeBeregninger/indmatprosjekt/prosjekt_2_utlevert_kode/layers.py:365\u001b[0m, in \u001b[0;36mEmbedPosition.step_adam\u001b[0;34m(self, alpha, j)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_adam\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha, j):\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed\u001b[38;5;241m.\u001b[39mstep_adam(alpha, j)\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VitenskapeligeBeregninger/indmatprosjekt/prosjekt_2_utlevert_kode/layers.py:43\u001b[0m, in \u001b[0;36mLayer.step_adam\u001b[0;34m(self, alpha, j)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_adam\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha, j):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# M_j = beta1*M_j-1 + (1-beta1)*G_j\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta2\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta2)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# V_j = beta2*V_j-1 + (1-beta2)*(G_j*G_J)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         m_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mj)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (30,7) (30,6) "
     ]
    }
   ],
   "source": [
    "losses = trainModel(nn,data, 150, loss, m, 10) #her kjem da dimensjonsfeil må sjekkes ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN IF NOT NEW TRAINED MODEL\n",
    "# with open(\"sortingTrained_v2\", 'wb') as f:\n",
    "    # pickle.dump(nn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The transformer model for sequence prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is all about *learning* useful *functions* from big *datasets*. These useful functions are called nevral networks, and are put together from smaller functions with parameters that are decided through optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.0** Structure of the datasets and the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Let          $a = 15$, $b = 7$, $c = 47$, $d = 152$\n",
    "\n",
    "then we have   $[1, 5, 7, 4, 7, 1, 5]$, $y =[1, 5, 2]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Let   \n",
    "\n",
    "$x^{(0)} = [1, 5, 7, 4, 7]$\n",
    "\n",
    "$x^{(1)} = [1, 5, 7, 4, 7, \\hat{z_4}]$\n",
    "\n",
    "$x^{(2)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}]$\n",
    "\n",
    "$x^{(3)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}, \\hat{z_6}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(0)}}, \\hat{z_1^{(0)}}, \\hat{z_2^{(0)}}, \\hat{z_3^{(0)}}, \\hat{z_4^{(0)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(1)}}, \\hat{z_1^{(1)}}, \\hat{z_2^{(1)}}, \\hat{z_3^{(1)}}, \\hat{z_4^{(1)}}, \\hat{z_5^{(1)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(2)}}, \\hat{z_1^{(2)}}, \\hat{z_2^{(2)}}, \\hat{z_3^{(2)}}, \\hat{z_4^{(2)}}, \\hat{z_5^{(2)}}, \\hat{z_6^{(2)}}]$\n",
    "\n",
    "If the optimization is good, the result should be:\n",
    "\n",
    "$\\hat{z_4^{(0)}} = 1, \\hat{z_5^{(1)}} = 5$ og $\\hat{z_6^{(2)}} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)**\n",
    "\n",
    "For the object function to be $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$, the probability distribution must be given by:\n",
    "\n",
    "$\\hat{Y} = onehot(y) = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "In this case $\\hat{y}$ will be given by:\n",
    "\n",
    "$\\hat{y} := argmax(\\hat{Y}) = y$\n",
    "\n",
    "Then, $\\mathcal(L) = 0$ will be fulfilled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)**\n",
    "\n",
    "The number of parameters is given by:\n",
    "\n",
    "$d(2m + n_{max} + L(4k + 2p))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)**\n",
    "\n",
    "$X = onehot(x) = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}, z_0 = W_Ex + [W_P]_{0:n} = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Z = softmax(\\begin{bmatrix}\n",
    "1\n",
    "\\alpha\n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "\\frac{e^1}{e^1+1^{\\alpha}} \\\\\n",
    "\\frac{e^{\\alpha}}{e^1+e^{\\alpha}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\hat{z} = 1 \\Rightarrow \\alpha > 1$ (when $\\alpha=1$, undefined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.0** Implementing the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** \n",
    "\n",
    "1) If the type of layer is identified as `LinearLayer` or `Attention`, `NeuralNetwork` will inherit `step_gd` from the `Layer` class. \n",
    "\n",
    "2) If the type of layer is identified as `EmbedPosition`, `NeuralNetwork` will inherit `step_gd` from the `EmbedPosition` class. \n",
    "\n",
    "3) If the type of layer is identified as `FeedForward`, `NeuralNetwork` will inherit `step_gd` from the `FeedForward` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from layers import *\n",
    "from training import trainModel\n",
    "import numpy as np\n",
    "from data_generators import get_train_test_addition, get_train_test_sorting\n",
    "from training import *\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5\n",
    "m = 2\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2*r-1\n",
    "sigma = Relu\n",
    "\n",
    "data = get_train_test_sorting(r,m,batchSize, batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed,att1, ff1,att2, ff2,un_embed,softmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  0.6956595803848478  gradient =  0.04207500656455133\n",
      "Iterasjon  1  L =  0.6845541476670784  gradient =  0.041636079648641515\n",
      "Iterasjon  2  L =  0.6740033352393731  gradient =  0.04122971640630487\n",
      "Iterasjon  3  L =  0.661933359077414  gradient =  0.04082882718283264\n",
      "Iterasjon  4  L =  0.6479653224990141  gradient =  0.04047794753349226\n",
      "Iterasjon  5  L =  0.632323869355111  gradient =  0.04025231905787785\n",
      "Iterasjon  6  L =  0.6159524200714492  gradient =  0.04023368164769431\n",
      "Iterasjon  7  L =  0.6000285874008491  gradient =  0.04044963577407931\n",
      "Iterasjon  8  L =  0.5853392484260124  gradient =  0.04075403374705236\n",
      "Iterasjon  9  L =  0.5714162179405105  gradient =  0.04092264568820327\n",
      "Iterasjon  10  L =  0.5579781681945948  gradient =  0.04094959981674154\n",
      "Iterasjon  11  L =  0.5454338575382557  gradient =  0.04092032974712575\n",
      "Iterasjon  12  L =  0.5338740266519955  gradient =  0.04091456493051925\n",
      "Iterasjon  13  L =  0.5236352360198268  gradient =  0.04105722286742214\n",
      "Iterasjon  14  L =  0.5152836160636939  gradient =  0.04140862942573782\n",
      "Iterasjon  15  L =  0.5087999653199486  gradient =  0.041913913431102894\n",
      "Iterasjon  16  L =  0.5039373197004251  gradient =  0.04251147210992634\n",
      "Iterasjon  17  L =  0.5003098478657921  gradient =  0.043168038490900645\n",
      "Iterasjon  18  L =  0.4974650522982693  gradient =  0.0438126152341965\n",
      "Iterasjon  19  L =  0.49513999864076147  gradient =  0.04442900530400272\n",
      "Iterasjon  20  L =  0.49321534644420106  gradient =  0.04498845944312921\n",
      "Iterasjon  21  L =  0.4916282124248865  gradient =  0.04552141100247528\n",
      "Iterasjon  22  L =  0.4903135654253578  gradient =  0.04608211972703578\n",
      "Iterasjon  23  L =  0.48920322970307417  gradient =  0.04662216704378533\n",
      "Iterasjon  24  L =  0.488295075697858  gradient =  0.04718045435702843\n",
      "Iterasjon  25  L =  0.4875320416128243  gradient =  0.04772959207693901\n",
      "Iterasjon  26  L =  0.48687014089114744  gradient =  0.048239893013220944\n",
      "Iterasjon  27  L =  0.4862852013487554  gradient =  0.048687789852797134\n",
      "Iterasjon  28  L =  0.4857627362068765  gradient =  0.049080531433539416\n",
      "Iterasjon  29  L =  0.4852887067941575  gradient =  0.04940322242591073\n",
      "Iterasjon  30  L =  0.48484201212793365  gradient =  0.049629051902591204\n",
      "Iterasjon  31  L =  0.484422403889394  gradient =  0.04979617802031233\n",
      "Iterasjon  32  L =  0.4840255038778801  gradient =  0.049933715409975155\n",
      "Iterasjon  33  L =  0.4836311832114838  gradient =  0.04999096706089786\n",
      "Iterasjon  34  L =  0.4832420651745492  gradient =  0.05001999150955036\n",
      "Iterasjon  35  L =  0.4828623742440349  gradient =  0.05003714912181728\n",
      "Iterasjon  36  L =  0.48249363578279203  gradient =  0.05005318985514843\n",
      "Iterasjon  37  L =  0.4821351888403199  gradient =  0.05002853690006152\n",
      "Iterasjon  38  L =  0.48174363621256966  gradient =  0.04993090133959621\n",
      "Iterasjon  39  L =  0.4813435835183551  gradient =  0.049856006239180135\n",
      "Iterasjon  40  L =  0.48094319635353716  gradient =  0.0498165893592303\n",
      "Iterasjon  41  L =  0.48049123478739986  gradient =  0.04973854279556273\n",
      "Iterasjon  42  L =  0.4800115879601636  gradient =  0.04961659583543192\n",
      "Iterasjon  43  L =  0.47944514428536616  gradient =  0.049427744697609396\n",
      "Iterasjon  44  L =  0.47878790376352887  gradient =  0.04922737388409834\n",
      "Iterasjon  45  L =  0.47812972014095745  gradient =  0.04913493562855455\n",
      "Iterasjon  46  L =  0.47754122205953725  gradient =  0.049079884154199255\n",
      "Iterasjon  47  L =  0.47706219724265664  gradient =  0.04905970023934251\n",
      "Iterasjon  48  L =  0.4766701368509964  gradient =  0.04905071853767544\n",
      "Iterasjon  49  L =  0.47635429391126927  gradient =  0.04901892945006844\n",
      "Iterasjon  50  L =  0.4760893319440629  gradient =  0.04890715872799988\n",
      "Iterasjon  51  L =  0.47586659525651454  gradient =  0.04871876948381854\n",
      "Iterasjon  52  L =  0.4756816279619672  gradient =  0.04849174616266843\n",
      "Iterasjon  53  L =  0.4755229253068455  gradient =  0.04824943614759086\n",
      "Iterasjon  54  L =  0.47537911125294247  gradient =  0.04798750493923828\n",
      "Iterasjon  55  L =  0.4752491666952152  gradient =  0.04771287581876832\n",
      "Iterasjon  56  L =  0.4751334254326939  gradient =  0.047456339411739656\n",
      "Iterasjon  57  L =  0.47503158528559775  gradient =  0.04722590663017461\n",
      "Iterasjon  58  L =  0.4749324202570314  gradient =  0.04696853814595797\n",
      "Iterasjon  59  L =  0.4748397259863598  gradient =  0.04672719783201538\n",
      "Iterasjon  60  L =  0.47475421717608646  gradient =  0.0465212202207593\n",
      "Iterasjon  61  L =  0.4746750226610287  gradient =  0.04634433956815926\n",
      "Iterasjon  62  L =  0.4746023015148074  gradient =  0.046194306631684785\n",
      "Iterasjon  63  L =  0.4745356302910805  gradient =  0.046063150183545835\n",
      "Iterasjon  64  L =  0.47447415699410744  gradient =  0.04595419386287647\n",
      "Iterasjon  65  L =  0.4744160524384632  gradient =  0.045857292342220975\n",
      "Iterasjon  66  L =  0.4743641414204315  gradient =  0.045775836813434986\n",
      "Iterasjon  67  L =  0.474317205047034  gradient =  0.0457131224862188\n",
      "Iterasjon  68  L =  0.4742734233676858  gradient =  0.04565685010245389\n",
      "Iterasjon  69  L =  0.47423147516258063  gradient =  0.04560536902352025\n",
      "Iterasjon  70  L =  0.4741930564816187  gradient =  0.04554882648855099\n",
      "Iterasjon  71  L =  0.4741558457876122  gradient =  0.04548336828295012\n",
      "Iterasjon  72  L =  0.47412363727739193  gradient =  0.045432809997499386\n",
      "Iterasjon  73  L =  0.47409153125683245  gradient =  0.04538282611387118\n",
      "Iterasjon  74  L =  0.4740633288492308  gradient =  0.04534530336743234\n",
      "Iterasjon  75  L =  0.47403592535970934  gradient =  0.04531264269057961\n",
      "Iterasjon  76  L =  0.4740107455341282  gradient =  0.045283713660045195\n",
      "Iterasjon  77  L =  0.4739865252508918  gradient =  0.04525947708739638\n",
      "Iterasjon  78  L =  0.4739615414746764  gradient =  0.04521742072179789\n",
      "Iterasjon  79  L =  0.4739424018416483  gradient =  0.04519898128268495\n",
      "Iterasjon  80  L =  0.47392067660278947  gradient =  0.0451735362414869\n",
      "Iterasjon  81  L =  0.47390069856653466  gradient =  0.045145303056917184\n",
      "Iterasjon  82  L =  0.47388205508410913  gradient =  0.04511426840799586\n",
      "Iterasjon  83  L =  0.47386511913024715  gradient =  0.04509745366106743\n",
      "Iterasjon  84  L =  0.4738484522562185  gradient =  0.04508653098393599\n",
      "Iterasjon  85  L =  0.47382966214401917  gradient =  0.04505133148382182\n",
      "Iterasjon  86  L =  0.4738188459599334  gradient =  0.045050228113249546\n",
      "Iterasjon  87  L =  0.47379977961834285  gradient =  0.04502435209406831\n",
      "Iterasjon  88  L =  0.4737878690806944  gradient =  0.04500840110444956\n",
      "Iterasjon  89  L =  0.47377469642194736  gradient =  0.04500564729829278\n",
      "Iterasjon  90  L =  0.4737605403738595  gradient =  0.0449655970858448\n",
      "Iterasjon  91  L =  0.47374982199867277  gradient =  0.04492158015318738\n",
      "Iterasjon  92  L =  0.4737377273170905  gradient =  0.04488352356265257\n",
      "Iterasjon  93  L =  0.4737266759408077  gradient =  0.04485991571731219\n",
      "Iterasjon  94  L =  0.4737187776369038  gradient =  0.0448687718622315\n",
      "Iterasjon  95  L =  0.4737046716251266  gradient =  0.044838950588799484\n",
      "Iterasjon  96  L =  0.47369631028064196  gradient =  0.0448264681320244\n",
      "Iterasjon  97  L =  0.47368763798420294  gradient =  0.044825607605117536\n",
      "Iterasjon  98  L =  0.47367800812667527  gradient =  0.04481288121301052\n",
      "Iterasjon  99  L =  0.4736707354318808  gradient =  0.04481128127548029\n",
      "Iterasjon  100  L =  0.47366077625910974  gradient =  0.044793672222151526\n",
      "Iterasjon  101  L =  0.47365458092628243  gradient =  0.044791483254982736\n",
      "Iterasjon  102  L =  0.47364672932429003  gradient =  0.04478399159091654\n",
      "Iterasjon  103  L =  0.47363834634483953  gradient =  0.044763265978267124\n",
      "Iterasjon  104  L =  0.473634072504763  gradient =  0.04476966036253786\n",
      "Iterasjon  105  L =  0.47362523664412953  gradient =  0.04475504073191795\n",
      "Iterasjon  106  L =  0.47362041401638366  gradient =  0.044752575986038906\n",
      "Iterasjon  107  L =  0.47361352325755457  gradient =  0.044742769121697466\n",
      "Iterasjon  108  L =  0.47360747581405516  gradient =  0.04473092229588066\n",
      "Iterasjon  109  L =  0.4736032030894862  gradient =  0.044734099610253666\n",
      "Iterasjon  110  L =  0.4735963752790167  gradient =  0.044722254796146124\n",
      "Iterasjon  111  L =  0.47359253005725144  gradient =  0.044720206106060964\n",
      "Iterasjon  112  L =  0.4735871342006289  gradient =  0.04471690740814069\n",
      "Iterasjon  113  L =  0.47358192936364124  gradient =  0.04470508980883777\n",
      "Iterasjon  114  L =  0.47357785261300905  gradient =  0.04470335051635949\n",
      "Iterasjon  115  L =  0.4735737509466671  gradient =  0.04470053434473665\n",
      "Iterasjon  116  L =  0.4735683998914816  gradient =  0.044687911797269435\n",
      "Iterasjon  117  L =  0.4735665820993171  gradient =  0.04469738170179203\n",
      "Iterasjon  118  L =  0.4735592974994122  gradient =  0.044677959049592396\n",
      "Iterasjon  119  L =  0.47355885376694495  gradient =  0.0446816886743442\n",
      "Iterasjon  120  L =  0.47355261240223656  gradient =  0.04467076347799587\n",
      "Iterasjon  121  L =  0.47355223399987245  gradient =  0.04468000263254829\n",
      "Iterasjon  122  L =  0.47354520564537317  gradient =  0.04466343237922245\n",
      "Iterasjon  123  L =  0.4735447960504618  gradient =  0.0446657390692803\n",
      "Iterasjon  124  L =  0.47354016074694805  gradient =  0.04467250777022159\n",
      "Iterasjon  125  L =  0.47353784449882375  gradient =  0.04466751001417119\n",
      "Iterasjon  126  L =  0.4735346514005784  gradient =  0.04466078326036185\n",
      "Iterasjon  127  L =  0.4735317459779119  gradient =  0.044646265986293905\n",
      "Iterasjon  128  L =  0.47352872948232616  gradient =  0.044639264600026346\n",
      "Iterasjon  129  L =  0.47352788602394613  gradient =  0.04465144878589286\n",
      "Iterasjon  130  L =  0.47352396702843585  gradient =  0.04465526647221173\n",
      "Iterasjon  131  L =  0.47352068811876924  gradient =  0.04464407252885934\n",
      "Iterasjon  132  L =  0.4735203010959116  gradient =  0.04464183019777627\n",
      "Iterasjon  133  L =  0.4735160000371951  gradient =  0.0446270497010654\n",
      "Iterasjon  134  L =  0.4735159084619121  gradient =  0.044633165273836574\n",
      "Iterasjon  135  L =  0.4735112080229299  gradient =  0.04462158354554932\n",
      "Iterasjon  136  L =  0.47351224394623176  gradient =  0.044650775810805114\n",
      "Iterasjon  137  L =  0.47350792473485637  gradient =  0.044671459306914366\n",
      "Iterasjon  138  L =  0.4735050490760214  gradient =  0.04464231696199836\n",
      "Iterasjon  139  L =  0.47350319986973355  gradient =  0.044618846218547274\n",
      "Iterasjon  140  L =  0.47350314800033083  gradient =  0.044636424279665195\n",
      "Iterasjon  141  L =  0.4734985117588277  gradient =  0.044620170844638304\n",
      "Iterasjon  142  L =  0.47350070636461894  gradient =  0.04463900625443113\n",
      "Iterasjon  143  L =  0.47349497900783205  gradient =  0.044621174623871836\n",
      "Iterasjon  144  L =  0.4734974386507149  gradient =  0.044640912470337746\n",
      "Iterasjon  145  L =  0.4734917798142139  gradient =  0.04461568454979344\n",
      "Iterasjon  146  L =  0.47349452996682906  gradient =  0.04461898857017083\n",
      "Iterasjon  147  L =  0.4734885821451913  gradient =  0.0446135839896305\n",
      "Iterasjon  148  L =  0.47349013337951645  gradient =  0.04462670842354404\n",
      "Iterasjon  149  L =  0.47348676501608383  gradient =  0.044615959895728745\n",
      "Iterasjon  150  L =  0.47348781941503104  gradient =  0.04463118675630206\n",
      "Iterasjon  151  L =  0.4734836489108526  gradient =  0.04460329521489945\n",
      "Iterasjon  152  L =  0.47348755873340237  gradient =  0.044658369794310655\n",
      "Iterasjon  153  L =  0.47348032666390594  gradient =  0.04461565270360691\n",
      "Iterasjon  154  L =  0.4734837940674005  gradient =  0.04463770119524974\n",
      "Iterasjon  155  L =  0.47347853237443543  gradient =  0.04460470297262414\n",
      "Iterasjon  156  L =  0.4734823847747389  gradient =  0.04463428817108679\n",
      "Iterasjon  157  L =  0.4734756220194182  gradient =  0.04459474134049498\n",
      "Iterasjon  158  L =  0.47348159436882326  gradient =  0.044649464809645635\n",
      "Iterasjon  159  L =  0.47347332215272353  gradient =  0.044603720538323385\n",
      "Iterasjon  160  L =  0.47347912000907255  gradient =  0.04462769575495124\n",
      "Iterasjon  161  L =  0.4734712666501245  gradient =  0.04457814785346142\n",
      "Iterasjon  162  L =  0.47347755802550145  gradient =  0.04462737102148797\n",
      "Iterasjon  163  L =  0.4734703093826197  gradient =  0.04459348036115332\n",
      "Iterasjon  164  L =  0.4734736227497721  gradient =  0.044604725774092885\n",
      "Iterasjon  165  L =  0.47347141260142045  gradient =  0.044620436213215715\n",
      "Iterasjon  166  L =  0.47347031579620297  gradient =  0.04459324981530903\n",
      "Iterasjon  167  L =  0.4734691576853497  gradient =  0.04459287234188743\n",
      "Iterasjon  168  L =  0.47347016616280324  gradient =  0.04460582344803816\n",
      "Iterasjon  169  L =  0.47346623817352285  gradient =  0.04457308195149842\n",
      "Iterasjon  170  L =  0.4734703994769438  gradient =  0.0446165639100738\n",
      "Iterasjon  171  L =  0.4734641551564476  gradient =  0.044585153060408216\n",
      "Iterasjon  172  L =  0.4734684134250756  gradient =  0.04460448553722161\n",
      "Iterasjon  173  L =  0.47346291527028805  gradient =  0.044568947433725296\n",
      "Iterasjon  174  L =  0.4734678494044694  gradient =  0.04461558497909104\n",
      "Iterasjon  175  L =  0.473460882390172  gradient =  0.044581426951386956\n",
      "Iterasjon  176  L =  0.47346480794082374  gradient =  0.04457163875986631\n",
      "Iterasjon  177  L =  0.4734625742309414  gradient =  0.04459951875299565\n",
      "Iterasjon  178  L =  0.4734606164727455  gradient =  0.044579383932800865\n",
      "Iterasjon  179  L =  0.4734631097328178  gradient =  0.04457120696097721\n",
      "Iterasjon  180  L =  0.4734584796717998  gradient =  0.04454143315636482\n",
      "Iterasjon  181  L =  0.47346436780207657  gradient =  0.04462466541828872\n",
      "Iterasjon  182  L =  0.4734561959041697  gradient =  0.04458023167279422\n",
      "Iterasjon  183  L =  0.4734608354214281  gradient =  0.04456185898596788\n",
      "Iterasjon  184  L =  0.47345796928482076  gradient =  0.04457071313315111\n",
      "Iterasjon  185  L =  0.47345661301123015  gradient =  0.04455455460782134\n",
      "Iterasjon  186  L =  0.473460859396282  gradient =  0.044620134723716315\n",
      "Iterasjon  187  L =  0.47345381305451334  gradient =  0.04456284967806927\n",
      "Iterasjon  188  L =  0.47345873319827475  gradient =  0.04455684182655687\n",
      "Iterasjon  189  L =  0.4734533654459783  gradient =  0.04452745703033697\n",
      "Iterasjon  190  L =  0.4734594865231586  gradient =  0.04460441543723177\n",
      "Iterasjon  191  L =  0.4734526345370139  gradient =  0.04457711992636309\n",
      "Iterasjon  192  L =  0.4734551723225561  gradient =  0.04454447560909322\n",
      "Iterasjon  193  L =  0.473454385987856  gradient =  0.044567670864173355\n",
      "Iterasjon  194  L =  0.4734526489159533  gradient =  0.04455547890440852\n",
      "Iterasjon  195  L =  0.47345489671163576  gradient =  0.04458856913602452\n",
      "Iterasjon  196  L =  0.47345079685819086  gradient =  0.04455239656477138\n",
      "Iterasjon  197  L =  0.4734548651065367  gradient =  0.044577372572817076\n",
      "Iterasjon  198  L =  0.4734503195922942  gradient =  0.044548553401458094\n",
      "Iterasjon  199  L =  0.47345393371693845  gradient =  0.044563978553511595\n",
      "Iterasjon  200  L =  0.47344966810535816  gradient =  0.04454119416259401\n",
      "Iterasjon  201  L =  0.4734533018062671  gradient =  0.044574939934150896\n",
      "Iterasjon  202  L =  0.4734492338942175  gradient =  0.04455064832184467\n",
      "Iterasjon  203  L =  0.4734508586696662  gradient =  0.044538250547547314\n",
      "Iterasjon  204  L =  0.47345177793135595  gradient =  0.04458885285778156\n",
      "Iterasjon  205  L =  0.47344816927440975  gradient =  0.044556765973903276\n",
      "Iterasjon  206  L =  0.4734528030948849  gradient =  0.044554204672313275\n",
      "Iterasjon  207  L =  0.4734459310766887  gradient =  0.04450713049047285\n",
      "Iterasjon  208  L =  0.47345402139746595  gradient =  0.04459871848838985\n",
      "Iterasjon  209  L =  0.47344620473428234  gradient =  0.04456269211024217\n",
      "Iterasjon  210  L =  0.4734493902381761  gradient =  0.04454462725671732\n",
      "Iterasjon  211  L =  0.4734478157301731  gradient =  0.044530877041857604\n",
      "Iterasjon  212  L =  0.47344877490025644  gradient =  0.04456206860626311\n",
      "Iterasjon  213  L =  0.4734470089773263  gradient =  0.044548336951287036\n",
      "Iterasjon  214  L =  0.47344778983995284  gradient =  0.04453469280388772\n",
      "Iterasjon  215  L =  0.47344814348624126  gradient =  0.04454869785058545\n",
      "Iterasjon  216  L =  0.47344581881566833  gradient =  0.04453179913610548\n",
      "Iterasjon  217  L =  0.4734507214573241  gradient =  0.0445776780132278\n",
      "Iterasjon  218  L =  0.47344354579870684  gradient =  0.04452770150022972\n",
      "Iterasjon  219  L =  0.4734497889647458  gradient =  0.04453904029789806\n",
      "Iterasjon  220  L =  0.4734448552121638  gradient =  0.04453583474679894\n",
      "Iterasjon  221  L =  0.4734457648335172  gradient =  0.04453006657948497\n",
      "Iterasjon  222  L =  0.473448275751958  gradient =  0.04457559883665984\n",
      "Iterasjon  223  L =  0.47344373135457624  gradient =  0.04453288665556504\n",
      "Iterasjon  224  L =  0.4734461332504841  gradient =  0.044521766464522744\n",
      "Iterasjon  225  L =  0.4734450645617413  gradient =  0.04452068577450891\n",
      "Iterasjon  226  L =  0.4734496557577475  gradient =  0.04460984039133218\n",
      "Iterasjon  227  L =  0.47344143043778564  gradient =  0.044544584135458316\n",
      "Iterasjon  228  L =  0.4734478986573928  gradient =  0.04452969101957155\n",
      "Iterasjon  229  L =  0.47344247388893657  gradient =  0.04450184047033223\n",
      "Iterasjon  230  L =  0.4734452362193037  gradient =  0.04451908311356995\n",
      "Iterasjon  231  L =  0.47344636097671844  gradient =  0.04457329122814844\n",
      "Iterasjon  232  L =  0.47344335850409636  gradient =  0.044538510489854716\n",
      "Iterasjon  233  L =  0.47344585977963083  gradient =  0.044564799563726006\n",
      "Iterasjon  234  L =  0.47344253697497407  gradient =  0.044530026255641925\n",
      "Iterasjon  235  L =  0.47344468199362194  gradient =  0.04451879509467335\n",
      "Iterasjon  236  L =  0.47344348841686246  gradient =  0.044514665220464794\n",
      "Iterasjon  237  L =  0.4734458537189704  gradient =  0.04456078226991369\n",
      "Iterasjon  238  L =  0.4734419023728537  gradient =  0.044531256443791516\n",
      "Iterasjon  239  L =  0.47344600090338734  gradient =  0.04452768992419934\n",
      "Iterasjon  240  L =  0.473441776339541  gradient =  0.04452241769345641\n",
      "Iterasjon  241  L =  0.47344392429792465  gradient =  0.04452763377685045\n",
      "Iterasjon  242  L =  0.4734432649077669  gradient =  0.04452233861318448\n",
      "Iterasjon  243  L =  0.47344421790297686  gradient =  0.04455282726130681\n",
      "Iterasjon  244  L =  0.47344202533487445  gradient =  0.04452839148859348\n",
      "Iterasjon  245  L =  0.47344331139558893  gradient =  0.044515216346742846\n",
      "Iterasjon  246  L =  0.47344209668146114  gradient =  0.04450648323117089\n",
      "Iterasjon  247  L =  0.47344709984123856  gradient =  0.04459127820871728\n",
      "Iterasjon  248  L =  0.4734392171880864  gradient =  0.04452686346756982\n",
      "Iterasjon  249  L =  0.4734465267809386  gradient =  0.044523010956363206\n",
      "Iterasjon  250  L =  0.473439517891274  gradient =  0.0444869816661305\n",
      "Iterasjon  251  L =  0.4734443033199532  gradient =  0.044528625452518025\n",
      "Iterasjon  252  L =  0.4734423167640416  gradient =  0.04453371311554957\n",
      "Iterasjon  253  L =  0.47344141781195176  gradient =  0.04450921966197824\n",
      "Iterasjon  254  L =  0.4734448911756496  gradient =  0.04457665017536094\n",
      "Iterasjon  255  L =  0.4734402450315233  gradient =  0.04453195735955801\n",
      "Iterasjon  256  L =  0.47344259921543513  gradient =  0.04450599225124356\n",
      "Iterasjon  257  L =  0.47344178433939765  gradient =  0.044503058437991\n",
      "Iterasjon  258  L =  0.4734429736382584  gradient =  0.04451834017139626\n",
      "Iterasjon  259  L =  0.4734408820880125  gradient =  0.0445100734140044\n",
      "Iterasjon  260  L =  0.4734437871700897  gradient =  0.044518289924970374\n",
      "Iterasjon  261  L =  0.47344162104633725  gradient =  0.04454089332140883\n",
      "Iterasjon  262  L =  0.4734415721540489  gradient =  0.04452444104899703\n",
      "Iterasjon  263  L =  0.47344258713794984  gradient =  0.04451595579976499\n",
      "Iterasjon  264  L =  0.4734420153759566  gradient =  0.04454765322590496\n",
      "Iterasjon  265  L =  0.4734406585165737  gradient =  0.044522809131755554\n",
      "Iterasjon  266  L =  0.4734426135511042  gradient =  0.044512146796733286\n",
      "Iterasjon  267  L =  0.4734408529922735  gradient =  0.04449824242950626\n",
      "Iterasjon  268  L =  0.47344341749872665  gradient =  0.04455002776561097\n",
      "Iterasjon  269  L =  0.47344055652653383  gradient =  0.044525175755731526\n",
      "Iterasjon  270  L =  0.4734432453792491  gradient =  0.044508069958170075\n",
      "Iterasjon  271  L =  0.47343960115797207  gradient =  0.044487828805185196\n",
      "Iterasjon  272  L =  0.4734433293146969  gradient =  0.04451428593776522\n",
      "Iterasjon  273  L =  0.4734414554446197  gradient =  0.04453527810561976\n",
      "Iterasjon  274  L =  0.47344070928554566  gradient =  0.044511751454902694\n",
      "Iterasjon  275  L =  0.47344198898556755  gradient =  0.044506499611988895\n",
      "Iterasjon  276  L =  0.4734408333020742  gradient =  0.04450000872502044\n",
      "Iterasjon  277  L =  0.4734436157808036  gradient =  0.044551818882933095\n",
      "Iterasjon  278  L =  0.4734395774102538  gradient =  0.04451370879397433\n",
      "Iterasjon  279  L =  0.47344281330954485  gradient =  0.04451853821685808\n",
      "Iterasjon  280  L =  0.473440144580746  gradient =  0.04450002303949283\n",
      "Iterasjon  281  L =  0.47344178696218986  gradient =  0.0445068827465905\n",
      "Iterasjon  282  L =  0.47344170175953976  gradient =  0.04450885755435683\n",
      "Iterasjon  283  L =  0.47344132487305546  gradient =  0.044524188814076236\n",
      "Iterasjon  284  L =  0.473441973039208  gradient =  0.04451548569985279\n",
      "Iterasjon  285  L =  0.4734408623828938  gradient =  0.04453442558295611\n",
      "Iterasjon  286  L =  0.4734395327599842  gradient =  0.04450767269088842\n",
      "Iterasjon  287  L =  0.4734412913009723  gradient =  0.04450285519792313\n",
      "Iterasjon  288  L =  0.4734393454921187  gradient =  0.044491727390311896\n",
      "Iterasjon  289  L =  0.4734405714644595  gradient =  0.044502136938556765\n",
      "Iterasjon  290  L =  0.47344035438140286  gradient =  0.0445074140012718\n",
      "Iterasjon  291  L =  0.4734400022271507  gradient =  0.04450079881101288\n",
      "Iterasjon  292  L =  0.4734429196408246  gradient =  0.044570433840688116\n",
      "Iterasjon  293  L =  0.473438238851681  gradient =  0.044516200522931974\n",
      "Iterasjon  294  L =  0.47344191649729606  gradient =  0.04449996499574666\n",
      "Iterasjon  295  L =  0.47343891117829695  gradient =  0.04448125785071224\n",
      "Iterasjon  296  L =  0.47344146910597046  gradient =  0.04450441972820652\n",
      "Iterasjon  297  L =  0.47344014703247694  gradient =  0.04450555692021938\n",
      "Iterasjon  298  L =  0.4734399347405054  gradient =  0.044496434088954824\n",
      "Iterasjon  299  L =  0.47344038844618674  gradient =  0.04449896609643553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = trainModel(nn,data,300,loss, m, 0.001)\n",
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "len(losses)\n",
    "plt.semilogy([i for i in range(len(losses))], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DO NOT RUN IF NOT NEW TRAINED MODEL\n",
    "# with open(\"sortingTrained_v1\", 'wb') as f:\n",
    "#     pickle.dump(nn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"savedObject\", 'rb') as f:\n",
    "     nn2 = pickle.load(f)\n",
    "\n",
    "type(nn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"sortingTrained_v1\", \"rb\") as f:\n",
    "#     nn = pickle.load(f)\n",
    "\n",
    "y_pred = predict(nn, data['x_test'], r, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print()\n",
    "print(data['y_test'])\n",
    "np.count_nonzero(np.count_nonzero(y_pred == data['y_test'], axis=2) == y_pred.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 7\n",
    "m = 5\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "iterations = 300\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2*r-1\n",
    "sigma = Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_train_test_sorting(r,m,batchSize, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed,att1,ff1,att2, ff2, un_embed,softmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainModel(nn,data, iterations, loss, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "batches = 20\n",
    "d = 30\n",
    "k=20\n",
    "p=40\n",
    "L=3\n",
    "m=10\n",
    "n_max = 2*2 + 3\n",
    "\n",
    "data =get_train_test_addition(2,batch_size,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "layers.append(embed)\n",
    "for i in range(L):\n",
    "    att1 = Attention(d,k)\n",
    "    ff1 = FeedForward(d,p)\n",
    "\n",
    "    layers.append(att1)\n",
    "    layers.append(ff1)\n",
    "\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "layers.append(un_embed)\n",
    "softmax = Softmax()\n",
    "layers.append(softmax)\n",
    "\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainModel(nn,data, 150, loss, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

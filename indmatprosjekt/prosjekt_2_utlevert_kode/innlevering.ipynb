{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The transformer model for sequence prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is all about *learning* useful *functions* from big *datasets*. These useful functions are called nevral networks, and are put together from smaller functions with parameters that are decided through optimization. In opposition to conventional programming, where we tell the computer what to do, nevral networks learns from observational data and figure out its own solution to the given problem. Here we will implement the transformer model, one of the main components in big languagemodels like *ChatGPT*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.0** Structure of the datasets and the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Let          $a = 15$, $b = 7$, $c = 47$, $d = 152$\n",
    "\n",
    "then we have   $[1, 5, 7, 4, 7, 1, 5]$, $y =[1, 5, 2]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Let   \n",
    "\n",
    "$x^{(0)} = [1, 5, 7, 4, 7]$\n",
    "\n",
    "$x^{(1)} = [1, 5, 7, 4, 7, \\hat{z_4}]$\n",
    "\n",
    "$x^{(2)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}]$\n",
    "\n",
    "$x^{(3)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}, \\hat{z_6}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(0)}}, \\hat{z_1^{(0)}}, \\hat{z_2^{(0)}}, \\hat{z_3^{(0)}}, \\hat{z_4^{(0)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(1)}}, \\hat{z_1^{(1)}}, \\hat{z_2^{(1)}}, \\hat{z_3^{(1)}}, \\hat{z_4^{(1)}}, \\hat{z_5^{(1)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(2)}}, \\hat{z_1^{(2)}}, \\hat{z_2^{(2)}}, \\hat{z_3^{(2)}}, \\hat{z_4^{(2)}}, \\hat{z_5^{(2)}}, \\hat{z_6^{(2)}}]$\n",
    "\n",
    "If the optimization is good, the result should be:\n",
    "\n",
    "$\\hat{z_4^{(0)}} = 1, \\hat{z_5^{(1)}} = 5$ og $\\hat{z_6^{(2)}} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)**\n",
    "\n",
    "For the object function to be $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$, the probability distribution must be given by:\n",
    "\n",
    "$\\hat{Y} = onehot(y) = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "In this case $\\hat{y}$ will be given by:\n",
    "\n",
    "$\\hat{y} := argmax(\\hat{Y}) = y$\n",
    "\n",
    "Then, $\\mathcal(L) = 0$ will be fulfilled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)**\n",
    "\n",
    "The number of parameters is given by:\n",
    "\n",
    "$d(2m + n_{max} + L(4k + 2p))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)**\n",
    "\n",
    "$X = onehot(x) = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}, z_0 = W_Ex + [W_P]_{0:n} = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Z = softmax(\\begin{bmatrix}\n",
    "1\n",
    "\\alpha\n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "\\frac{e^1}{e^1+1^{\\alpha}} \\\\\n",
    "\\frac{e^{\\alpha}}{e^1+e^{\\alpha}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\hat{z} = 1 \\Rightarrow \\alpha > 1$ (when $\\alpha=1$, undefined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.0** Implementing the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** \n",
    "\n",
    "1) If the type of layer is identified as `LinearLayer` or `Attention`, `NeuralNetwork` will inherit `step_gd` from the `Layer` class. \n",
    "\n",
    "2) If the type of layer is identified as `EmbedPosition`, `NeuralNetwork` will inherit `step_gd` from the `EmbedPosition` class. \n",
    "\n",
    "3) If the type of layer is identified as `FeedForward`, `NeuralNetwork` will inherit `step_gd` from the `FeedForward` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from layers import *\n",
    "from training import trainModel\n",
    "import numpy as np\n",
    "from data_generators import get_train_test_addition, get_train_test_sorting\n",
    "from training import *\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5\n",
    "m = 2\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2*r-1\n",
    "sigma = Relu\n",
    "\n",
    "data = get_train_test_sorting(r,m,batchSize, batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "att_ffd_list = []\n",
    "for layer in range(L):\n",
    "    att = Attention(d,k)\n",
    "    ff = FeedForward(d,p)\n",
    "    att_ffd_list.append(att)\n",
    "    att_ffd_list.append(ff)\n",
    "\n",
    "layers = [embed] + att_ffd_list + [un_embed] + [softmax]\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  0.6836031011071302  gradient =  0.030811637502761467\n",
      "Iterasjon  1  L =  0.6609392387805296  gradient =  0.03016372040119741\n",
      "Iterasjon  2  L =  0.6368598561805274  gradient =  0.02949538104620711\n",
      "Iterasjon  3  L =  0.6091394895767538  gradient =  0.02880880813777376\n",
      "Iterasjon  4  L =  0.5772968175454415  gradient =  0.028140065803586637\n",
      "Iterasjon  5  L =  0.5412011418684349  gradient =  0.02758456224907873\n",
      "Iterasjon  6  L =  0.5019614738735344  gradient =  0.027320877285910893\n",
      "Iterasjon  7  L =  0.46263247728038176  gradient =  0.027592088589963728\n",
      "Iterasjon  8  L =  0.4267740142847086  gradient =  0.02856477238786068\n",
      "Iterasjon  9  L =  0.3967322765785524  gradient =  0.030128513038069096\n",
      "Iterasjon  10  L =  0.37287381122026464  gradient =  0.03167075217707321\n",
      "Iterasjon  11  L =  0.35432117898949966  gradient =  0.032636547722656215\n",
      "Iterasjon  12  L =  0.3400131398182541  gradient =  0.033002396000049616\n",
      "Iterasjon  13  L =  0.3289921518003286  gradient =  0.03301827724117993\n",
      "Iterasjon  14  L =  0.3201579001077569  gradient =  0.03285759644636759\n",
      "Iterasjon  15  L =  0.3127445518013335  gradient =  0.03261317045561763\n",
      "Iterasjon  16  L =  0.30628015446373114  gradient =  0.03227018370719939\n",
      "Iterasjon  17  L =  0.30056948574402553  gradient =  0.03185093199397009\n",
      "Iterasjon  18  L =  0.2954479137690169  gradient =  0.0313964439794678\n",
      "Iterasjon  19  L =  0.2906908733014895  gradient =  0.030852809664375754\n",
      "Iterasjon  20  L =  0.28605187126206805  gradient =  0.03024895919496924\n",
      "Iterasjon  21  L =  0.28139204131993845  gradient =  0.029639659828540818\n",
      "Iterasjon  22  L =  0.2759796447917127  gradient =  0.02876651936683996\n",
      "Iterasjon  23  L =  0.2686667930920031  gradient =  0.02786863813359826\n",
      "Iterasjon  24  L =  0.26153598453380733  gradient =  0.027965015770628817\n",
      "Iterasjon  25  L =  0.2540260199466301  gradient =  0.028038914620592246\n",
      "Iterasjon  26  L =  0.24578690576757758  gradient =  0.02735876680125919\n",
      "Iterasjon  27  L =  0.23664781667608628  gradient =  0.02663167571834132\n",
      "Iterasjon  28  L =  0.22581194108961014  gradient =  0.025661020604253886\n",
      "Iterasjon  29  L =  0.21343628627274783  gradient =  0.02459185025269995\n",
      "Iterasjon  30  L =  0.19910835171535188  gradient =  0.023391675767260922\n",
      "Iterasjon  31  L =  0.18154051710312383  gradient =  0.021574522643671212\n",
      "Iterasjon  32  L =  0.15953732219941724  gradient =  0.019814187891670533\n",
      "Iterasjon  33  L =  0.13221149245388697  gradient =  0.018629092855160755\n",
      "Iterasjon  34  L =  0.10310635960225081  gradient =  0.017753427098549632\n",
      "Iterasjon  35  L =  0.07424242994019016  gradient =  0.01703995990135125\n",
      "Iterasjon  36  L =  0.04965060389984151  gradient =  0.01653545659602867\n",
      "Iterasjon  37  L =  0.03189686204779079  gradient =  0.016214688682182152\n",
      "Iterasjon  38  L =  0.02082001304702515  gradient =  0.0160397841125209\n",
      "Iterasjon  39  L =  0.01468589560586446  gradient =  0.015944934120007914\n",
      "Iterasjon  40  L =  0.011192976814287025  gradient =  0.01589284297143769\n",
      "Iterasjon  41  L =  0.009015706038105377  gradient =  0.01586062934629187\n",
      "Iterasjon  42  L =  0.007501485873414722  gradient =  0.015834555263765124\n",
      "Iterasjon  43  L =  0.006397830546898908  gradient =  0.015818664214164304\n",
      "Iterasjon  44  L =  0.005627768649612138  gradient =  0.015805854390317967\n",
      "Iterasjon  45  L =  0.005046542710087034  gradient =  0.01579603008265672\n",
      "Iterasjon  46  L =  0.00454469296798351  gradient =  0.01578738742456455\n",
      "Iterasjon  47  L =  0.0041060480795341026  gradient =  0.015779799330029768\n",
      "Iterasjon  48  L =  0.0037296976602120223  gradient =  0.015773544235448512\n",
      "Iterasjon  49  L =  0.0034380354583564707  gradient =  0.015768553199555283\n",
      "Iterasjon  50  L =  0.0031877897733814014  gradient =  0.015764285871371807\n",
      "Iterasjon  51  L =  0.0029671052297085736  gradient =  0.015760573121629092\n",
      "Iterasjon  52  L =  0.0027711381517962668  gradient =  0.01575728427659876\n",
      "Iterasjon  53  L =  0.0025955100206385587  gradient =  0.01575432199806431\n",
      "Iterasjon  54  L =  0.0024381312982147962  gradient =  0.015751655511875334\n",
      "Iterasjon  55  L =  0.0022963500872365543  gradient =  0.015749266899126178\n",
      "Iterasjon  56  L =  0.0021671932623696663  gradient =  0.015747071482263876\n",
      "Iterasjon  57  L =  0.002048733279847215  gradient =  0.01574508445633164\n",
      "Iterasjon  58  L =  0.0019387349198156747  gradient =  0.015743249606391677\n",
      "Iterasjon  59  L =  0.0018392764864707805  gradient =  0.015741621372564258\n",
      "Iterasjon  60  L =  0.00174670992487812  gradient =  0.015740114668432677\n",
      "Iterasjon  61  L =  0.0016617051153408012  gradient =  0.015738736211370893\n",
      "Iterasjon  62  L =  0.0015827217880579926  gradient =  0.015737456926437736\n",
      "Iterasjon  63  L =  0.0015098038493564594  gradient =  0.015736286427796765\n",
      "Iterasjon  64  L =  0.0014415760475136302  gradient =  0.015735194418803278\n",
      "Iterasjon  65  L =  0.0013775280396073448  gradient =  0.015734161284667324\n",
      "Iterasjon  66  L =  0.0013185219514821218  gradient =  0.015733227866706098\n",
      "Iterasjon  67  L =  0.001262449473532457  gradient =  0.015732340967992264\n",
      "Iterasjon  68  L =  0.0012100208098317616  gradient =  0.01573151424385943\n",
      "Iterasjon  69  L =  0.001160691168736805  gradient =  0.01573074234244979\n",
      "Iterasjon  70  L =  0.0011139820233792837  gradient =  0.015730012085209157\n",
      "Iterasjon  71  L =  0.0010699365453589822  gradient =  0.015729325938705793\n",
      "Iterasjon  72  L =  0.001028296004532156  gradient =  0.01572868023939144\n",
      "Iterasjon  73  L =  0.0009888036635297558  gradient =  0.015728070122853856\n",
      "Iterasjon  74  L =  0.0009506038914338834  gradient =  0.0157274649239756\n",
      "Iterasjon  75  L =  0.0009163595829214859  gradient =  0.015726959945795014\n",
      "Iterasjon  76  L =  0.0008808468304874443  gradient =  0.015726396629699633\n",
      "Iterasjon  77  L =  0.0008499972481905094  gradient =  0.015725941558696447\n",
      "Iterasjon  78  L =  0.0008178620715626542  gradient =  0.015725439503806882\n",
      "Iterasjon  79  L =  0.0007895406412017752  gradient =  0.015725022396052356\n",
      "Iterasjon  80  L =  0.0007603496072803725  gradient =  0.01572456836544119\n",
      "Iterasjon  81  L =  0.0007339354419765465  gradient =  0.015724168299596542\n",
      "Iterasjon  82  L =  0.0007082259301844516  gradient =  0.015723786592742574\n",
      "Iterasjon  83  L =  0.0006834236236299504  gradient =  0.015723411304357313\n",
      "Iterasjon  84  L =  0.0006599605334466831  gradient =  0.015723059908661272\n",
      "Iterasjon  85  L =  0.0006373872722989099  gradient =  0.015722724249261206\n",
      "Iterasjon  86  L =  0.0006156875183074603  gradient =  0.015722400719777877\n",
      "Iterasjon  87  L =  0.0005949189458190304  gradient =  0.01572209226584175\n",
      "Iterasjon  88  L =  0.0005752654042969988  gradient =  0.015721810375448526\n",
      "Iterasjon  89  L =  0.0005560731355343176  gradient =  0.015721534091093226\n",
      "Iterasjon  90  L =  0.0005379581036989176  gradient =  0.015721281816769312\n",
      "Iterasjon  91  L =  0.0005203457790720772  gradient =  0.015721029574454846\n",
      "Iterasjon  92  L =  0.0005033352243267066  gradient =  0.01572077586494752\n",
      "Iterasjon  93  L =  0.00048767173032703644  gradient =  0.01572055581166661\n",
      "Iterasjon  94  L =  0.0004719477564857297  gradient =  0.015720331136164147\n",
      "Iterasjon  95  L =  0.00045743057793780275  gradient =  0.015720140902448727\n",
      "Iterasjon  96  L =  0.0004427044609281343  gradient =  0.0157199182696632\n",
      "Iterasjon  97  L =  0.0004293497782007975  gradient =  0.015719740804150742\n",
      "Iterasjon  98  L =  0.00041574770299587565  gradient =  0.015719526558947275\n",
      "Iterasjon  99  L =  0.00040337074171096273  gradient =  0.01571935495175781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = trainModel(nn,data,100,loss, m, r, 0.001)\n",
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11de99390>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "len(losses)\n",
    "plt.semilogy([i for i in range(len(losses))], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DO NOT RUN IF NOT NEW TRAINED MODEL\n",
    "# with open(\"sortingTrained_v1\", 'wb') as f:\n",
    "    # pickle.dump(nn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"savedObject\", 'rb') as f:\n",
    "     #nn2 = pickle.load(f)\n",
    "\n",
    "#type(nn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(250, 5)\n",
      "(250, 5)\n",
      "(250, 1)\n",
      "1\n",
      "(250, 6)\n",
      "(250, 6)\n",
      "(250, 1)\n",
      "2\n",
      "(250, 7)\n",
      "(250, 7)\n",
      "(250, 1)\n",
      "3\n",
      "(250, 8)\n",
      "(250, 8)\n",
      "(250, 1)\n",
      "4\n",
      "(250, 9)\n",
      "(250, 9)\n",
      "(250, 1)\n"
     ]
    }
   ],
   "source": [
    "# with open(\"sortingTrained_v1\", \"rb\") as f:\n",
    "#     nn = pickle.load(f)\n",
    "\n",
    "y_pred = predict(nn, data['x_test'], r, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 1. 1. 1.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. 1. 1.]\n",
      "  [0. 0. 0. 1. 1.]\n",
      "  [0. 0. 1. 1. 1.]]]\n",
      "\n",
      "[[[0. 0. 1. 1. 1.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. 1. 1.]\n",
      "  [0. 0. 0. 1. 1.]\n",
      "  [0. 0. 1. 1. 1.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print()\n",
    "print(data['y_test'])\n",
    "np.count_nonzero(np.count_nonzero(y_pred == data['y_test'], axis=2) == y_pred.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 7\n",
    "m = 5\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "iterations = 100\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2*r-1\n",
    "sigma = Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_train_test_sorting(r,m,batchSize, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed,att1,ff1,att2, ff2, un_embed,softmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  1.3180256645356145  gradient =  1.3605222554664682\n",
      "Iterasjon  1  L =  0.8029755259139074  gradient =  0.06154618129138519\n",
      "Iterasjon  2  L =  0.667250975058661  gradient =  0.04949780787098209\n",
      "Iterasjon  3  L =  0.5072564028433587  gradient =  0.04171710821499383\n",
      "Iterasjon  4  L =  0.33895783953587555  gradient =  0.03706066356420531\n",
      "Iterasjon  5  L =  0.22619707285197427  gradient =  0.026535261543089276\n",
      "Iterasjon  6  L =  0.1518116783863584  gradient =  0.01877311459919807\n",
      "Iterasjon  7  L =  0.10090941903367305  gradient =  0.020665271450939694\n",
      "Iterasjon  8  L =  0.06117139354637643  gradient =  0.017002306628716556\n",
      "Iterasjon  9  L =  0.0415024979300117  gradient =  0.01417552459519387\n",
      "Iterasjon  10  L =  0.026955114252082867  gradient =  0.013496925384970307\n",
      "Iterasjon  11  L =  0.017882066274081305  gradient =  0.013422863177501812\n",
      "Iterasjon  12  L =  0.013592010588700049  gradient =  0.013184414916341057\n",
      "Iterasjon  13  L =  0.00959545858703111  gradient =  0.013036842091560745\n",
      "Iterasjon  14  L =  0.007301502475513225  gradient =  0.013009653348738444\n",
      "Iterasjon  15  L =  0.006009369949758164  gradient =  0.012973813468684314\n",
      "Iterasjon  16  L =  0.005873924233109644  gradient =  0.013072541993283669\n",
      "Iterasjon  17  L =  0.005318683569327401  gradient =  0.013164510579015878\n",
      "Iterasjon  18  L =  0.003870981278336028  gradient =  0.012933707366023573\n",
      "Iterasjon  19  L =  0.003390873975882807  gradient =  0.01293774405863989\n",
      "Iterasjon  20  L =  0.002970875829712891  gradient =  0.012921170682557504\n",
      "Iterasjon  21  L =  0.0025034482323506605  gradient =  0.012903932108580509\n",
      "Iterasjon  22  L =  0.0022824714371413634  gradient =  0.0129021560074658\n",
      "Iterasjon  23  L =  0.0020210825333771064  gradient =  0.012896708921371253\n",
      "Iterasjon  24  L =  0.0018845565349152795  gradient =  0.012895319173808727\n",
      "Iterasjon  25  L =  0.0017217902568510662  gradient =  0.012890510484606982\n",
      "Iterasjon  26  L =  0.0016003835582656827  gradient =  0.012888933009817152\n",
      "Iterasjon  27  L =  0.001494267153284519  gradient =  0.012886963478114057\n",
      "Iterasjon  28  L =  0.0014013357028983234  gradient =  0.012885808367653701\n",
      "Iterasjon  29  L =  0.0013133084273750182  gradient =  0.012884171675389777\n",
      "Iterasjon  30  L =  0.001238709681154568  gradient =  0.012883234542352271\n",
      "Iterasjon  31  L =  0.0011736719504077695  gradient =  0.012882090055701817\n",
      "Iterasjon  32  L =  0.0011093894544362008  gradient =  0.012881269978761739\n",
      "Iterasjon  33  L =  0.0010511301204794742  gradient =  0.012880630366191546\n",
      "Iterasjon  34  L =  0.000995820690085481  gradient =  0.01288024518528979\n",
      "Iterasjon  35  L =  0.0009428805776449769  gradient =  0.012879965685476604\n",
      "Iterasjon  36  L =  0.0008894750102233191  gradient =  0.012879587042390658\n",
      "Iterasjon  37  L =  0.0008376804553336606  gradient =  0.012878883136491447\n",
      "Iterasjon  38  L =  0.0007948158164672936  gradient =  0.012878260923058148\n",
      "Iterasjon  39  L =  0.0007630914186903659  gradient =  0.01287780974222269\n",
      "Iterasjon  40  L =  0.0007377348719122875  gradient =  0.012877452577127994\n",
      "Iterasjon  41  L =  0.0007127914097861859  gradient =  0.012877109075238832\n",
      "Iterasjon  42  L =  0.0006861587730696312  gradient =  0.012876928657551707\n",
      "Iterasjon  43  L =  0.0006595020235065096  gradient =  0.012876877529473195\n",
      "Iterasjon  44  L =  0.0006280645316205194  gradient =  0.012876759285639049\n",
      "Iterasjon  45  L =  0.0005965058651126124  gradient =  0.012876452752273355\n",
      "Iterasjon  46  L =  0.0005692138578100761  gradient =  0.012876148178478725\n",
      "Iterasjon  47  L =  0.0005503626816130199  gradient =  0.012875997983445301\n",
      "Iterasjon  48  L =  0.0005203649796236278  gradient =  0.012875811248661646\n",
      "Iterasjon  49  L =  0.0004907288213313062  gradient =  0.012875696391560972\n",
      "Iterasjon  50  L =  0.00046446705500640306  gradient =  0.012875529040534843\n",
      "Iterasjon  51  L =  0.0004361263063483332  gradient =  0.012875521922728766\n",
      "Iterasjon  52  L =  0.00040705959814669437  gradient =  0.012875256706934976\n",
      "Iterasjon  53  L =  0.0003793267432449677  gradient =  0.012875058613495813\n",
      "Iterasjon  54  L =  0.0003577094485710308  gradient =  0.012874775593150317\n",
      "Iterasjon  55  L =  0.00034038702364072804  gradient =  0.012874623029272716\n",
      "Iterasjon  56  L =  0.0003237220652810357  gradient =  0.012874501811671329\n",
      "Iterasjon  57  L =  0.00030649498881185177  gradient =  0.012874377978031525\n",
      "Iterasjon  58  L =  0.0002913591172075379  gradient =  0.012874280271468601\n",
      "Iterasjon  59  L =  0.00027660278351209204  gradient =  0.012874163912447502\n",
      "Iterasjon  60  L =  0.0002650082067874707  gradient =  0.012874039786069134\n",
      "Iterasjon  61  L =  0.00025572225526857977  gradient =  0.01287394010426177\n",
      "Iterasjon  62  L =  0.00024819682610594006  gradient =  0.012873847706556156\n",
      "Iterasjon  63  L =  0.0002402463669905107  gradient =  0.012873773079302361\n",
      "Iterasjon  64  L =  0.00023110927902562318  gradient =  0.012873704753296947\n",
      "Iterasjon  65  L =  0.00022112376122020025  gradient =  0.01287363660872464\n",
      "Iterasjon  66  L =  0.00021054835462432116  gradient =  0.01287357395605488\n",
      "Iterasjon  67  L =  0.00019916170142572713  gradient =  0.01287353928891103\n",
      "Iterasjon  68  L =  0.00019103553263654974  gradient =  0.012873419786156179\n",
      "Iterasjon  69  L =  0.00018317371310977768  gradient =  0.012873326708927142\n",
      "Iterasjon  70  L =  0.00017638348403712384  gradient =  0.012873236803306225\n",
      "Iterasjon  71  L =  0.00016893128198658218  gradient =  0.012873176258722023\n",
      "Iterasjon  72  L =  0.0001612226831383032  gradient =  0.01287312583475272\n",
      "Iterasjon  73  L =  0.0001548157841358167  gradient =  0.012873073415079855\n",
      "Iterasjon  74  L =  0.0001497949612404465  gradient =  0.012873029740963167\n",
      "Iterasjon  75  L =  0.00014539253127469673  gradient =  0.012872984887263765\n",
      "Iterasjon  76  L =  0.0001406610606052273  gradient =  0.01287294208041331\n",
      "Iterasjon  77  L =  0.00013592483890357274  gradient =  0.012872894177644362\n",
      "Iterasjon  78  L =  0.00013141246606818713  gradient =  0.012872849786735962\n",
      "Iterasjon  79  L =  0.00012714807272990494  gradient =  0.012872812526859784\n",
      "Iterasjon  80  L =  0.00012309989776071033  gradient =  0.012872778871224087\n",
      "Iterasjon  81  L =  0.00011942086701450781  gradient =  0.012872746235451121\n",
      "Iterasjon  82  L =  0.00011595053292019644  gradient =  0.012872713033759817\n",
      "Iterasjon  83  L =  0.00011246810638017987  gradient =  0.01287268000022065\n",
      "Iterasjon  84  L =  0.00010915559136145704  gradient =  0.012872648784354469\n",
      "Iterasjon  85  L =  0.00010595843220922382  gradient =  0.01287261839421967\n",
      "Iterasjon  86  L =  0.00010287231629157422  gradient =  0.012872591843681835\n",
      "Iterasjon  87  L =  9.998444054918601e-05  gradient =  0.012872565750111392\n",
      "Iterasjon  88  L =  9.725416327320075e-05  gradient =  0.012872540175498687\n",
      "Iterasjon  89  L =  9.45492798893366e-05  gradient =  0.012872514514297112\n",
      "Iterasjon  90  L =  9.188350778369754e-05  gradient =  0.012872490697195605\n",
      "Iterasjon  91  L =  8.941161644592132e-05  gradient =  0.012872468378476212\n",
      "Iterasjon  92  L =  8.703013233958714e-05  gradient =  0.012872446243380305\n",
      "Iterasjon  93  L =  8.470707826496111e-05  gradient =  0.012872424977791518\n",
      "Iterasjon  94  L =  8.242303354407017e-05  gradient =  0.012872404142831638\n",
      "Iterasjon  95  L =  8.029496170105397e-05  gradient =  0.012872384313407164\n",
      "Iterasjon  96  L =  7.825283398680298e-05  gradient =  0.01287236667673534\n",
      "Iterasjon  97  L =  7.623666245698836e-05  gradient =  0.01287234889530398\n",
      "Iterasjon  98  L =  7.43390231711421e-05  gradient =  0.012872331182854801\n",
      "Iterasjon  99  L =  7.245339396410933e-05  gradient =  0.012872314918892748\n"
     ]
    }
   ],
   "source": [
    "losses = trainModel(nn,data, iterations, loss, m, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(250, 7)\n",
      "(250, 7)\n",
      "(250, 1)\n",
      "1\n",
      "(250, 8)\n",
      "(250, 8)\n",
      "(250, 1)\n",
      "2\n",
      "(250, 9)\n",
      "(250, 9)\n",
      "(250, 1)\n",
      "3\n",
      "(250, 10)\n",
      "(250, 10)\n",
      "(250, 1)\n",
      "4\n",
      "(250, 11)\n",
      "(250, 11)\n",
      "(250, 1)\n",
      "5\n",
      "(250, 12)\n",
      "(250, 12)\n",
      "(250, 1)\n",
      "6\n",
      "(250, 13)\n",
      "(250, 13)\n",
      "(250, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(nn, data['x_test'], r, m)\n",
    "np.count_nonzero(np.count_nonzero(y_pred == data['y_test'], axis=2) == y_pred.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "batches = 20\n",
    "d = 30\n",
    "k=20\n",
    "p=40\n",
    "L=3\n",
    "m=10\n",
    "n_max = 2*2 + 3\n",
    "\n",
    "data =get_train_test_addition(2,batch_size,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "layers.append(embed)\n",
    "for i in range(L):\n",
    "    att1 = Attention(d,k)\n",
    "    ff1 = FeedForward(d,p)\n",
    "\n",
    "    layers.append(att1)\n",
    "    layers.append(ff1)\n",
    "\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "layers.append(un_embed)\n",
    "softmax = Softmax()\n",
    "layers.append(softmax)\n",
    "\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (30,7) (30,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#her kjem da dimensjonsfeil må sjekkes ut\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VitenskapeligeBeregninger/indmatprosjekt/prosjekt_2_utlevert_kode/training.py:26\u001b[0m, in \u001b[0;36mtrainModel\u001b[0;34m(nn, data, iterations, loss, m, slice_number, step_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m         dLdZ \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m         nn\u001b[38;5;241m.\u001b[39mbackward(dLdZ)\n\u001b[0;32m---> 26\u001b[0m         \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterasjon \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(j), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m L = \u001b[39m\u001b[38;5;124m\"\u001b[39m,np\u001b[38;5;241m.\u001b[39mmean(losses[j,:]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m gradient = \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(dLdZ))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(losses, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/VitenskapeligeBeregninger/indmatprosjekt/prosjekt_2_utlevert_kode/neural_network.py:49\u001b[0m, in \u001b[0;36mNeuralNetwork.step_adam\u001b[0;34m(self, alpha, j)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m#Check if layer is of class a class that has parameters\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer,(LinearLayer,EmbedPosition,FeedForward,Attention)):\n\u001b[0;32m---> 49\u001b[0m         \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VitenskapeligeBeregninger/indmatprosjekt/prosjekt_2_utlevert_kode/layers.py:364\u001b[0m, in \u001b[0;36mEmbedPosition.step_adam\u001b[0;34m(self, alpha, j)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_adam\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha, j):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed\u001b[38;5;241m.\u001b[39mstep_adam(alpha, j)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VitenskapeligeBeregninger/indmatprosjekt/prosjekt_2_utlevert_kode/layers.py:43\u001b[0m, in \u001b[0;36mLayer.step_adam\u001b[0;34m(self, alpha, j)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_adam\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha, j):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# M_j = beta1*M_j-1 + (1-beta1)*G_j\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta2\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta2)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# V_j = beta2*V_j-1 + (1-beta2)*(G_j*G_J)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         m_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mj)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[param][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (30,7) (30,6) "
     ]
    }
   ],
   "source": [
    "losses = trainModel(nn,data, 150, loss, m, 3) #her kjem da dimensjonsfeil må sjekkes ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN IF NOT NEW TRAINED MODEL\n",
    "# with open(\"sortingTrained_v2\", 'wb') as f:\n",
    "    # pickle.dump(nn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The transformer model for sequence prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is all about *learning* useful *functions* from big *datasets*. These useful functions are called neural networks, and are put together from smaller functions with parameters that are decided through optimization. In opposition to conventional programming, where we tell the computer what to do, neural networks learns from observational data and figure out its own solution to the given problem. Here we will implement the transformer model, one of the main components in big languagemodels like *ChatGPT*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.0** Structure of the datasets and the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Let          $a = 15$, $b = 7$, $c = 47$, $d = 152$\n",
    "\n",
    "then we have   $x = [1, 5, 7, 4, 7, 1, 5]$, $y =[1, 5, 2]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Let   \n",
    "\n",
    "$x^{(0)} = [1, 5, 7, 4, 7]$\n",
    "\n",
    "$x^{(1)} = [1, 5, 7, 4, 7, \\hat{z_4}]$\n",
    "\n",
    "$x^{(2)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}]$\n",
    "\n",
    "$x^{(3)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}, \\hat{z_6}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(0)}}, \\hat{z_1^{(0)}}, \\hat{z_2^{(0)}}, \\hat{z_3^{(0)}}, \\hat{z_4^{(0)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(1)}}, \\hat{z_1^{(1)}}, \\hat{z_2^{(1)}}, \\hat{z_3^{(1)}}, \\hat{z_4^{(1)}}, \\hat{z_5^{(1)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(2)}}, \\hat{z_1^{(2)}}, \\hat{z_2^{(2)}}, \\hat{z_3^{(2)}}, \\hat{z_4^{(2)}}, \\hat{z_5^{(2)}}, \\hat{z_6^{(2)}}]$\n",
    "\n",
    "If the optimization is good, the result should be:\n",
    "\n",
    "$\\hat{z_4^{(0)}} = 1, \\hat{z_5^{(1)}} = 5$ og $\\hat{z_6^{(2)}} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)**\n",
    "\n",
    "For the object function to be $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$, the probability distribution must be given by:\n",
    "\n",
    "$\\hat{Y} = onehot(y) = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "In this case $\\hat{y}$ will be given by:\n",
    "\n",
    "$\\hat{y} := argmax(\\hat{Y}) = y$\n",
    "\n",
    "Then, $\\mathcal{L} = 0$ will be fulfilled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)**\n",
    "\n",
    "The number of parameters is given by:\n",
    "\n",
    "$d(2m + n_{max} + L(4k + 2p))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)**\n",
    "\n",
    "$X = \\mathrm{onehot}(x) = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}, z_0 = W_Ex + [W_P]_{0:n} = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Z = \\mathrm{softmax}\\left(\\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}\\right) = \\begin{bmatrix}\n",
    "\\frac{e^1}{e^1+1^{\\alpha}} \\\\\n",
    "\\frac{e^{\\alpha}}{e^1+e^{\\alpha}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\hat{z} = 1 \\Rightarrow \\alpha > 1$ (when $\\alpha=1$, undefined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.0** Implementing the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** noe m√• fikses\n",
    "\n",
    "\n",
    "1) If the type of layer is identified as `LinearLayer` or `Attention`, `NeuralNetwork` will inherit `step_gd` from the `Layer` class. \n",
    "\n",
    "2) If the type of layer is identified as `EmbedPosition`, `NeuralNetwork` will inherit `step_gd` from the `EmbedPosition` class. \n",
    "\n",
    "3) If the type of layer is identified as `FeedForward`, `NeuralNetwork` will inherit `step_gd` from the `FeedForward` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3(1)** Sorting of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from layers import *\n",
    "from training import trainModel\n",
    "from data_generators import get_train_test_addition, get_train_test_sorting\n",
    "from training import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for 0-1 sorting\n",
    "r = 5\n",
    "m = 2\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2*r-1\n",
    "sigma = Relu\n",
    "\n",
    "data = get_train_test_sorting(r,m,batchSize, batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sorting_01_Trained_loss\", 'rb') as f:\n",
    "    losses = pickle.load(f)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Training Plot for 0-1 Sorting\", fontsize=15)\n",
    "plt.xlabel(\"Number of Iterations\", fontsize=13)\n",
    "plt.ylabel(\"Loss\", fontsize=13)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.semilogy(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sorting_01_Trained_nn\", 'rb') as f:\n",
    "    nn = pickle.load(f)\n",
    "\n",
    "y_pred = predict(nn, data['x_test'],r, m)\n",
    "\n",
    "# If all elements equal, correct\n",
    "correct = np.count_nonzero(np.count_nonzero(y_pred == data['y_test'], axis = 2) == y_pred.shape[-1])\n",
    "all = y_pred.shape[0] * y_pred.shape[1]\n",
    "print(\"Testing results of 0-1 sorting:\")\n",
    "print(f\"# of correct predictions: {correct}, # of tests: {all}, percentage: {correct/all*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3(2)** Sorting of larger list with more possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for regular sorting\n",
    "r = 7\n",
    "m = 5\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "iterations = 100\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2*r-1\n",
    "\n",
    "data = get_train_test_sorting(r,m,batchSize, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sorting_Reg_Trained_loss\", 'rb') as f:\n",
    "    losses = pickle.load(f)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Training Plot for 0-1 Sorting\", fontsize=15)\n",
    "plt.xlabel(\"Number of Iterations\", fontsize=13)\n",
    "plt.ylabel(\"Loss\", fontsize=13)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.semilogy(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sorting_Reg_Trained_nn\", 'rb') as f:\n",
    "    nn = pickle.load(f)\n",
    "\n",
    "y_pred = predict(nn, data['x_test'],r, m)\n",
    "\n",
    "# If all elements equal, correct\n",
    "correct = np.count_nonzero(np.count_nonzero(y_pred == data['y_test'], axis = 2) == y_pred.shape[-1])\n",
    "all = y_pred.shape[0] * y_pred.shape[1]\n",
    "print(\"Testing results of 0-1 sorting:\")\n",
    "print(f\"# of correct predictions: {correct}, # of tests: {all}, percentage: {correct/all*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4** Addition of 2-digit numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "batches = 20\n",
    "d = 30\n",
    "r = 3\n",
    "k = 20\n",
    "p = 40\n",
    "L = 3\n",
    "m = 10\n",
    "n_max = 2*2 + 3\n",
    "\n",
    "data = get_train_test_addition(2,batch_size,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"additionTrained1_loss\", 'rb') as f:\n",
    "    losses = pickle.load(f)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Training Plot for 0-1 Sorting\", fontsize=15)\n",
    "plt.xlabel(\"Number of Iterations\", fontsize=13)\n",
    "plt.ylabel(\"Loss\", fontsize=13)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.semilogy(losses)\n",
    "\n",
    "print(losses[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"additionTrained1_nn\", 'rb') as f:\n",
    "    nn = pickle.load(f)\n",
    "\n",
    "y_pred = predict(nn, data['x_test'],r, m)\n",
    "\n",
    "# If all elements equal, correct\n",
    "correct = np.count_nonzero(np.count_nonzero(y_pred[:,:,::-1] == data['y_test'], axis = 2) == y_pred.shape[-1])\n",
    "all = y_pred.shape[0] * y_pred.shape[1]\n",
    "print(\"Testing results of 0-1 sorting:\")\n",
    "print(f\"# of correct predictions: {correct}, # of tests: {all}, percentage: {correct/all*100}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

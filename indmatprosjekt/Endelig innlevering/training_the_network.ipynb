{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.1** Training the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing `trainModel`, we can now use it to optimize the parameters. First we train for `r=5, m=2, samples=250, batches=10, d=10, k=5, p=15, L=2, n_max=2r-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from layers import *\n",
    "from training import trainModel\n",
    "import numpy as np\n",
    "from data_generators import get_train_test_addition, get_train_test_sorting\n",
    "from training import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5\n",
    "m = 2\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2 * r - 1\n",
    "sigma = Relu\n",
    "\n",
    "data = get_train_test_sorting(r, m, batchSize, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max, m,d)\n",
    "att1 = Attention(d, k)\n",
    "att2 = Attention(d, k)\n",
    "ff1 = FeedForward(d, p)\n",
    "ff2 = FeedForward(d, p)\n",
    "un_embed = LinearLayer(d, m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "att_ffd_list = []\n",
    "for layer in range(L):\n",
    "    att = Attention(d, k)\n",
    "    ff = FeedForward(d, p)\n",
    "    att_ffd_list.append(att)\n",
    "    att_ffd_list.append(ff)\n",
    "\n",
    "layers = [embed] + att_ffd_list + [un_embed] + [softmax]\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  0.6926300054945349  gradient =  0.031177046278907466\n",
      "Iterasjon  1  L =  0.6756082837814515  gradient =  0.030658131140889697\n",
      "Iterasjon  2  L =  0.6576486604305174  gradient =  0.030117004835031314\n",
      "Iterasjon  3  L =  0.6370337862828539  gradient =  0.029528791121772986\n",
      "Iterasjon  4  L =  0.6125853613952879  gradient =  0.02889088811204829\n",
      "Iterasjon  5  L =  0.5836124071509057  gradient =  0.028240784984939177\n",
      "Iterasjon  6  L =  0.5501311724158942  gradient =  0.027644684879366055\n",
      "Iterasjon  7  L =  0.512885819061402  gradient =  0.027231970196389538\n",
      "Iterasjon  8  L =  0.4745208136269185  gradient =  0.02715633508852183\n",
      "Iterasjon  9  L =  0.4379801164492195  gradient =  0.02750143375749827\n",
      "Iterasjon  10  L =  0.4053526872137615  gradient =  0.02819540166032849\n",
      "Iterasjon  11  L =  0.3774444935670441  gradient =  0.02888735753777892\n",
      "Iterasjon  12  L =  0.3536976155678948  gradient =  0.029198503534837415\n",
      "Iterasjon  13  L =  0.33387259913305134  gradient =  0.029360647237482166\n",
      "Iterasjon  14  L =  0.3184510363539447  gradient =  0.02972076467144131\n",
      "Iterasjon  15  L =  0.3064633097743835  gradient =  0.030039977114187703\n",
      "Iterasjon  16  L =  0.29655988491693674  gradient =  0.029877473911835938\n",
      "Iterasjon  17  L =  0.28761562188254974  gradient =  0.02940098537484936\n",
      "Iterasjon  18  L =  0.27908820607461987  gradient =  0.028707476639932325\n",
      "Iterasjon  19  L =  0.2708118741402563  gradient =  0.027983228495428378\n",
      "Iterasjon  20  L =  0.2620597365192393  gradient =  0.026893603854018438\n",
      "Iterasjon  21  L =  0.25258387390616405  gradient =  0.0258017759618525\n",
      "Iterasjon  22  L =  0.24232822807310392  gradient =  0.024813320156838962\n",
      "Iterasjon  23  L =  0.23060665085105034  gradient =  0.02387804938959317\n",
      "Iterasjon  24  L =  0.21672249507597727  gradient =  0.0229236842932229\n",
      "Iterasjon  25  L =  0.19984171007496315  gradient =  0.02182920360621857\n",
      "Iterasjon  26  L =  0.17905194954555287  gradient =  0.020591654527277434\n",
      "Iterasjon  27  L =  0.1543179833781959  gradient =  0.01932427750559153\n",
      "Iterasjon  28  L =  0.1245300642758667  gradient =  0.018126978007475897\n",
      "Iterasjon  29  L =  0.092363514947235  gradient =  0.01729509916074041\n",
      "Iterasjon  30  L =  0.06405857278340207  gradient =  0.016735217279131995\n",
      "Iterasjon  31  L =  0.04246991801468156  gradient =  0.01636768829446561\n",
      "Iterasjon  32  L =  0.027769985328253848  gradient =  0.016125056324358657\n",
      "Iterasjon  33  L =  0.018524381327734752  gradient =  0.015984921071199\n",
      "Iterasjon  34  L =  0.01300182257475738  gradient =  0.01590914655180233\n",
      "Iterasjon  35  L =  0.009814515736579685  gradient =  0.01586638139692207\n",
      "Iterasjon  36  L =  0.007845965839447905  gradient =  0.015838832148658933\n",
      "Iterasjon  37  L =  0.0065595467955940736  gradient =  0.0158187374699036\n",
      "Iterasjon  38  L =  0.005659088569860287  gradient =  0.01580559496983107\n",
      "Iterasjon  39  L =  0.005002990546784321  gradient =  0.01579546558646936\n",
      "Iterasjon  40  L =  0.004491723568723924  gradient =  0.015787093516614396\n",
      "Iterasjon  41  L =  0.004080130052587656  gradient =  0.015780715535750077\n",
      "Iterasjon  42  L =  0.0037387512676130496  gradient =  0.01577504142761187\n",
      "Iterasjon  43  L =  0.003450059709131853  gradient =  0.015770484476249853\n",
      "Iterasjon  44  L =  0.0032020249708243132  gradient =  0.01576631644919542\n",
      "Iterasjon  45  L =  0.00298652507104168  gradient =  0.01576294060085472\n",
      "Iterasjon  46  L =  0.0027966214885218736  gradient =  0.015759806498277334\n",
      "Iterasjon  47  L =  0.0026271264999385706  gradient =  0.015757003197066573\n",
      "Iterasjon  48  L =  0.0024753941079753584  gradient =  0.01575450115102701\n",
      "Iterasjon  49  L =  0.0023382660331544903  gradient =  0.01575222650514487\n",
      "Iterasjon  50  L =  0.0022137792460274236  gradient =  0.01575022517614637\n",
      "Iterasjon  51  L =  0.0021006185298227064  gradient =  0.015748466104190256\n",
      "Iterasjon  52  L =  0.00199625748214057  gradient =  0.015746755484404153\n",
      "Iterasjon  53  L =  0.001899863703942808  gradient =  0.015745148529139705\n",
      "Iterasjon  54  L =  0.0018103809432652718  gradient =  0.015743625034430095\n",
      "Iterasjon  55  L =  0.0017269400023621722  gradient =  0.015742267831088595\n",
      "Iterasjon  56  L =  0.0016497289836048547  gradient =  0.015740944508311896\n",
      "Iterasjon  57  L =  0.0015760806119981085  gradient =  0.015739695188172222\n",
      "Iterasjon  58  L =  0.0015081272004956385  gradient =  0.0157385682860264\n",
      "Iterasjon  59  L =  0.0014433693024951681  gradient =  0.015737441607036034\n",
      "Iterasjon  60  L =  0.001382181649026414  gradient =  0.015736416191309756\n",
      "Iterasjon  61  L =  0.0013249655308506204  gradient =  0.01573541903034623\n",
      "Iterasjon  62  L =  0.0012699240664388466  gradient =  0.015734530369734732\n",
      "Iterasjon  63  L =  0.0012183278491331622  gradient =  0.01573358232361175\n",
      "Iterasjon  64  L =  0.0011686280084897262  gradient =  0.015732817847804028\n",
      "Iterasjon  65  L =  0.0011220728364797843  gradient =  0.01573194221666942\n",
      "Iterasjon  66  L =  0.0010768845306936333  gradient =  0.01573127987405984\n",
      "Iterasjon  67  L =  0.0010348518303470013  gradient =  0.0157304430642212\n",
      "Iterasjon  68  L =  0.000993273762991654  gradient =  0.015729820459581625\n",
      "Iterasjon  69  L =  0.0009560575275623798  gradient =  0.01572905982015271\n",
      "Iterasjon  70  L =  0.0009182045342061544  gradient =  0.01572852749380402\n",
      "Iterasjon  71  L =  0.0008833237663407555  gradient =  0.01572780491126285\n",
      "Iterasjon  72  L =  0.0008502101735803692  gradient =  0.015727388425620745\n",
      "Iterasjon  73  L =  0.0008185646747387246  gradient =  0.015726854286419057\n",
      "Iterasjon  74  L =  0.0007882669201269146  gradient =  0.01572640076703139\n",
      "Iterasjon  75  L =  0.0007593490976600407  gradient =  0.0157258623046796\n",
      "Iterasjon  76  L =  0.0007315391023346272  gradient =  0.015725491973573823\n",
      "Iterasjon  77  L =  0.0007052208507777149  gradient =  0.01572493708792261\n",
      "Iterasjon  78  L =  0.0006795822525430808  gradient =  0.01572463994768895\n",
      "Iterasjon  79  L =  0.000655175834175749  gradient =  0.015724118408552284\n",
      "Iterasjon  80  L =  0.0006317362830725811  gradient =  0.015723839032310463\n",
      "Iterasjon  81  L =  0.0006089753772928735  gradient =  0.015723402070893552\n",
      "Iterasjon  82  L =  0.0005874428593499486  gradient =  0.01572306701901667\n",
      "Iterasjon  83  L =  0.0005665752722178343  gradient =  0.015722727051761868\n",
      "Iterasjon  84  L =  0.0005466029306384387  gradient =  0.01572238365867962\n",
      "Iterasjon  85  L =  0.0005274357443250529  gradient =  0.015722070241256682\n",
      "Iterasjon  86  L =  0.0005089782729550279  gradient =  0.01572176636026019\n",
      "Iterasjon  87  L =  0.0004912527964924418  gradient =  0.015721470199886525\n",
      "Iterasjon  88  L =  0.0004742183526460046  gradient =  0.01572118850161831\n",
      "Iterasjon  89  L =  0.00045783937274525884  gradient =  0.01572091760363766\n",
      "Iterasjon  90  L =  0.0004420941704716908  gradient =  0.015720657694907873\n",
      "Iterasjon  91  L =  0.00042695518812985427  gradient =  0.01572040789281999\n",
      "Iterasjon  92  L =  0.000412426503382471  gradient =  0.01572017406329732\n",
      "Iterasjon  93  L =  0.00039874790652501865  gradient =  0.015720003569126586\n",
      "Iterasjon  94  L =  0.0003854854034269496  gradient =  0.01571979684887114\n",
      "Iterasjon  95  L =  0.0003729284878005934  gradient =  0.015719607380971265\n",
      "Iterasjon  96  L =  0.00036087565323669285  gradient =  0.015719404975282805\n",
      "Iterasjon  97  L =  0.00034931491480403015  gradient =  0.015719210119049733\n",
      "Iterasjon  98  L =  0.0003381756278129014  gradient =  0.015719028141906663\n",
      "Iterasjon  99  L =  0.0003274329363077381  gradient =  0.01571885114757331\n"
     ]
    }
   ],
   "source": [
    "losses = trainModel(nn, data, 100, loss, m, r, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sortingTrained1\", 'wb') as f:\n",
    "    pickle.dump(nn, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

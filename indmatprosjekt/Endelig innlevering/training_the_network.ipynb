{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.1** Training the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing `trainModel`, we can now use it to optimize the parameters. First we train for `r=5, m=2, samples=250, batches=10, d=10, k=5, p=15, L=2, n_max=2r-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from layers import *\n",
    "from training import trainModel\n",
    "from data_generators import get_train_test_addition, get_train_test_sorting\n",
    "from training import *\n",
    "\n",
    "\n",
    "import pickle #Library for saving python objects for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 5\n",
    "m = 2\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2 * r - 1\n",
    "\n",
    "data = get_train_test_sorting(r, m, batchSize, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max, m,d)\n",
    "att1 = Attention(d, k)\n",
    "att2 = Attention(d, k)\n",
    "ff1 = FeedForward(d, p)\n",
    "ff2 = FeedForward(d, p)\n",
    "un_embed = LinearLayer(d, m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "att_ffd_list = []\n",
    "for layer in range(L):\n",
    "    att = Attention(d, k)\n",
    "    ff = FeedForward(d, p)\n",
    "    att_ffd_list.append(att)\n",
    "    att_ffd_list.append(ff)\n",
    "\n",
    "layers = [embed] + att_ffd_list + [un_embed] + [softmax]\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainModel(nn, data, 100, loss, m, r, 0.001)\n",
    "\n",
    "filename=\"sorting_01_Trained\" #Set the filename here\n",
    "\n",
    "with open(f\"{filename}_nn\", 'wb') as f:\n",
    "    pickle.dump(nn, f)\n",
    "\n",
    "with open(f\"{filename}_loss\", 'wb') as f:\n",
    "    pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we also train sorting for `r=7, m=5, samples=250, batches=10, d=20, k=10, p=25, L=2, n_max=2r-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 7\n",
    "m = 5\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "iterations = 100\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2 * r - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_train_test_sorting(r,m,batchSize, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed,att1,ff1,att2, ff2, un_embed,softmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainModel(nn,data, iterations, loss, m, r)\n",
    "\n",
    "filename=\"sorting_Reg_Trained\" #Set the filename here\n",
    "\n",
    "with open(f\"{filename}_nn\", 'wb') as f:\n",
    "    pickle.dump(nn, f)\n",
    "\n",
    "with open(f\"{filename}_loss\", 'wb') as f:\n",
    "    pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the addition we use `r=3, m=10, samples=250, batches=20, d=30, k=20, p=40, L=3, n_max=2*2 + 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "batches = 20\n",
    "d = 30\n",
    "k=20\n",
    "p=40\n",
    "L=3\n",
    "m=10\n",
    "n_max = 2*2 + 3\n",
    "\n",
    "data =get_train_test_addition(2,batch_size,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "\n",
    "embed = EmbedPosition(n_max - 1,m,d)\n",
    "layers.append(embed)\n",
    "for i in range(L):\n",
    "    att1 = Attention(d,k)\n",
    "    ff1 = FeedForward(d,p)\n",
    "\n",
    "    layers.append(att1)\n",
    "    layers.append(ff1)\n",
    "\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "layers.append(un_embed)\n",
    "softmax = Softmax()\n",
    "layers.append(softmax)\n",
    "\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainModel(nn,data, 150, loss, m, 3, 0.005) \n",
    "\n",
    "filename=\"additionTrained1\" #Set the filename here\n",
    "\n",
    "with open(f\"{filename}_nn\", 'wb') as f:\n",
    "    pickle.dump(nn, f)\n",
    "\n",
    "with open(f\"{filename}_loss\", 'wb') as f:\n",
    "    pickle.dump(losses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

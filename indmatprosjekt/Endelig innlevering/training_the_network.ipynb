{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.1** Training the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing `trainModel`, we can now use it to optimize the parameters. The first problem is to sort lists of length $5$, with possible element values of $0$ and $1$. For this we use two sets of `FeedForward` and `Attention` layers. We will call this 0-1 sorting from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from layers import *\n",
    "from training import trainModel\n",
    "from data_generators import get_train_test_addition, get_train_test_sorting\n",
    "from training import *\n",
    "\n",
    "\n",
    "import pickle #Library for saving python objects for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for 0-1-sorting\n",
    "r = 5\n",
    "m = 2\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2 * r - 1\n",
    "\n",
    "data = get_train_test_sorting(r, m, batchSize, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max, m,d)\n",
    "att1 = Attention(d, k)\n",
    "att2 = Attention(d, k)\n",
    "ff1 = FeedForward(d, p)\n",
    "ff2 = FeedForward(d, p)\n",
    "un_embed = LinearLayer(d, m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "att_ffd_list = []\n",
    "for layer in range(L):\n",
    "    att = Attention(d, k)\n",
    "    ff = FeedForward(d, p)\n",
    "    att_ffd_list.append(att)\n",
    "    att_ffd_list.append(ff)\n",
    "\n",
    "layers = [embed] + att_ffd_list + [un_embed] + [softmax]\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainModel(nn, data, 100, loss, m, r, 0.001)\n",
    "\n",
    "filename=\"sorting_01_Trained\" #Set the filename here\n",
    "# Uncomment to save to files\n",
    "# with open(f\"{filename}_nn\", 'wb') as f:\n",
    "    # pickle.dump(nn, f)\n",
    "\n",
    "# with open(f\"{filename}_loss\", 'wb') as f:\n",
    "    # pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train for sorting of lists with length $7$ and $5$ different values for the elements. For this we use two sets of `FeedForward` and `Attention` layers. We will call this regular sort from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 7\n",
    "m = 5\n",
    "batchSize = 250\n",
    "batches = 10\n",
    "iterations = 100\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2 * r - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_train_test_sorting(r,m,batchSize, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed,att1,ff1,att2, ff2, un_embed,softmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainModel(nn,data, iterations, loss, m, r)\n",
    "\n",
    "filename=\"sorting_Reg_Trained\" #Set the filename here\n",
    "\n",
    "# Uncomment to save to files\n",
    "# with open(f\"{filename}_nn\", 'wb') as f:\n",
    "    # pickle.dump(nn, f)\n",
    "\n",
    "# with open(f\"{filename}_loss\", 'wb') as f:\n",
    "    # pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know train it for addition of 2-digit numbers. For this we use three sets of `FeedForward` and `Attention` layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "batches = 20\n",
    "d = 30\n",
    "k=20\n",
    "p=40\n",
    "L=3\n",
    "m=10\n",
    "n_max = 2*2 + 3\n",
    "\n",
    "data =get_train_test_addition(2,batch_size,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "\n",
    "embed = EmbedPosition(n_max - 1,m,d)\n",
    "layers.append(embed)\n",
    "for i in range(L):\n",
    "    att1 = Attention(d,k)\n",
    "    ff1 = FeedForward(d,p)\n",
    "\n",
    "    layers.append(att1)\n",
    "    layers.append(ff1)\n",
    "\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "layers.append(un_embed)\n",
    "softmax = Softmax()\n",
    "layers.append(softmax)\n",
    "\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainModel(nn,data, 150, loss, m, 3, 0.005) \n",
    "\n",
    "filename=\"additionTrained1\" #Set the filename here\n",
    "\n",
    "# Uncomment to save to files\n",
    "# with open(f\"{filename}_nn\", 'wb') as f:\n",
    "#     pickle.dump(nn, f)\n",
    "\n",
    "# with open(f\"{filename}_loss\", 'wb') as f:\n",
    "#     pickle.dump(losses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

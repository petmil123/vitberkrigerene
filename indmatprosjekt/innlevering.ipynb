{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The transformer model for sequence prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is all about *learning* useful *functions* from big *datasets*. These useful functions are called nevral networks, and are put together from smaller functions with parameters that are decided through optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.0** Structure of the datasets and the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Let          $a = 15$, $b = 7$, $c = 47$, $d = 152$\n",
    "\n",
    "then we have   $[1, 5, 7, 4, 7, 1, 5]$, $y =[1, 5, 2]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Let   \n",
    "\n",
    "$x^{(0)} = [1, 5, 7, 4, 7]$\n",
    "\n",
    "$x^{(1)} = [1, 5, 7, 4, 7, \\hat{z_4}]$\n",
    "\n",
    "$x^{(2)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}]$\n",
    "\n",
    "$x^{(3)} = [1, 5, 7, 4, 7, \\hat{z_4}, \\hat{z_5}, \\hat{z_6}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(0)}}, \\hat{z_1^{(0)}}, \\hat{z_2^{(0)}}, \\hat{z_3^{(0)}}, \\hat{z_4^{(0)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(1)}}, \\hat{z_1^{(1)}}, \\hat{z_2^{(1)}}, \\hat{z_3^{(1)}}, \\hat{z_4^{(1)}}, \\hat{z_5^{(1)}}]$\n",
    "\n",
    "$f_{\\theta}(x^{(0)}) = [\\hat{z_0^{(2)}}, \\hat{z_1^{(2)}}, \\hat{z_2^{(2)}}, \\hat{z_3^{(2)}}, \\hat{z_4^{(2)}}, \\hat{z_5^{(2)}}, \\hat{z_6^{(2)}}]$\n",
    "\n",
    "If the optimization is good, the result should be:\n",
    "\n",
    "$\\hat{z_4^{(0)}} = 1, \\hat{z_5^{(1)}} = 5$ og $\\hat{z_6^{(2)}} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)**\n",
    "\n",
    "For the object function to be $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$, the probability distribution must be given by:\n",
    "\n",
    "$\\hat{Y} = onehot(y) = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "In this case $\\hat{y}$ will be given by:\n",
    "\n",
    "$\\hat{y} := argmax(\\hat{Y}) = y$\n",
    "\n",
    "Then, $\\mathcal(L) = 0$ will be fulfilled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)**\n",
    "\n",
    "The number of parameters is given by:\n",
    "\n",
    "$d(2m + n_{max} + L(4k + 2p))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)**\n",
    "\n",
    "$X = onehot(x) = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}, z_0 = W_Ex + [W_P]_{0:n} = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Z = softmax(\\begin{bmatrix}\n",
    "1\n",
    "\\alpha\n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "\\frac{e^1}{e^1+1^{\\alpha}} \\\\\n",
    "\\frac{e^{\\alpha}}{e^1+e^{\\alpha}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\hat{z} = 1 \\Rightarrow \\alpha > 1$ (when $\\alpha=1$, undefined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.0** Implementing the transformermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** \n",
    "\n",
    "1) If the type of layer is identified as `LinearLayer` or `Attention`, `NeuralNetwork` will inherit `step_gd` from the `Layer` class. \n",
    "\n",
    "2) If the type of layer is identified as `EmbedPosition`, `NeuralNetwork` will inherit `step_gd` from the `EmbedPosition` class. \n",
    "\n",
    "3) If the type of layer is identified as `FeedForward`, `NeuralNetwork` will inherit `step_gd` from the `FeedForward` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
